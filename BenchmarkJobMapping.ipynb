{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b92892-a93e-4bc9-9135-f7c5ff6e2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating taxonomy with jobs from O*Net\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"\"\n",
    "BASE_URL = \"\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "INPUT_FILE = \"job_tasks.csv\"\n",
    "OUTPUT_FILE = \"job_tasks_with_skills.csv\"\n",
    "TAXONOMY_FILE = \"taxonomy.json\"\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI Client\n",
    "# -------------------------------\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Add a persistent unique TaskID if not already there\n",
    "if \"TaskID\" not in df.columns:\n",
    "    df.insert(0, \"TaskID\", range(1, len(df) + 1))\n",
    "\n",
    "# Ensure required columns exist\n",
    "required = {\"Task\", \"Automation Desire Rating\", \"Job Security Rating\", \"Enjoyment Rating\", \"Occupation (O*NET-SOC Title)\"}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError(f\"CSV must contain columns: {required}\")\n",
    "\n",
    "# Add new column for skills if missing\n",
    "if \"generic_skill\" not in df.columns:\n",
    "    df[\"generic_skill\"] = None\n",
    "\n",
    "# -------------------------------\n",
    "# Load or init taxonomy\n",
    "# -------------------------------\n",
    "if os.path.exists(TAXONOMY_FILE):\n",
    "    with open(TAXONOMY_FILE, \"r\") as f:\n",
    "        taxonomy = json.load(f)\n",
    "else:\n",
    "    taxonomy = {\"root\": {}}\n",
    "\n",
    "# -------------------------------\n",
    "# Utility: atomic file write\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, filename):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False) as tmp:\n",
    "        json.dump(data, tmp, indent=2)\n",
    "        tmp_path = tmp.name\n",
    "    shutil.move(tmp_path, filename)\n",
    "\n",
    "def atomic_write_csv(df, filename):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".csv\") as tmp:\n",
    "        df.to_csv(tmp.name, index=False)\n",
    "        tmp_path = tmp.name\n",
    "    shutil.move(tmp_path, filename)\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt function\n",
    "# -------------------------------\n",
    "def classify_tasks(batch, taxonomy):\n",
    "    \"\"\"Send a batch of tasks to the LLM and return skill mappings + updated taxonomy.\"\"\"\n",
    "\n",
    "    tasks_str = \"\\n\".join(\n",
    "        [f\"ID:{row['TaskID']} - {row['Task']} ({row['Occupation (O*NET-SOC Title)']})\"\n",
    "         for _, row in batch.iterrows()]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in occupational task analysis.\n",
    "\n",
    "Here is the current taxonomy (JSON):\n",
    "<taxonomy>\n",
    "{json.dumps(taxonomy, indent=2)}\n",
    "</taxonomy>\n",
    "\n",
    "Here are new job tasks with job titles and IDs:\n",
    "{tasks_str}\n",
    "\n",
    "Instructions:\n",
    "1. For each task, provide a generic skill description that captures the core action, while preserving necessary domain nuance.\n",
    "   - Example: \"Maintain records of drilling operations\" → \"Maintain technical records\"\n",
    "   - Example: \"Review legal case documents\" → \"Review long formal documents\"\n",
    "2. Place each skill under the most relevant existing category in the taxonomy.\n",
    "3. If no suitable category exists, add a new subcategory under the most relevant parent.\n",
    "4. Output two sections:\n",
    "   <skills>\n",
    "   - ID:xxx → Generic skill (taxonomy_path)\n",
    "   </skills>\n",
    "\n",
    "   <updated_taxonomy>\n",
    "   {{...json updated taxonomy...}}\n",
    "   </updated_taxonomy>\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at building skill taxonomies.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            content = response.choices[0].message.content if response.choices else \"\"\n",
    "            if not content:\n",
    "                raise ValueError(\"Empty response\")\n",
    "            print(content)\n",
    "\n",
    "            # ----------------- Parse skills -----------------\n",
    "            skills_map = {}\n",
    "            if \"<skills>\" in content and \"</skills>\" in content:\n",
    "                skills_section = content.split(\"<skills>\")[1].split(\"</skills>\")[0].strip()\n",
    "                for line in skills_section.splitlines():\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\"- ID:\") or line.startswith(\"ID:\"):\n",
    "                        try:\n",
    "                            id_part, skill_part = line.replace(\"-\", \"\").split(\"→\", 1)\n",
    "                            task_id = int(id_part.strip().split(\":\")[1])\n",
    "                            skills_map[task_id] = skill_part.strip()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            # ----------------- Parse taxonomy -----------------\n",
    "            updated_taxonomy = taxonomy\n",
    "            if \"<updated_taxonomy>\" in content and \"</updated_taxonomy>\" in content:\n",
    "                taxonomy_section = content.split(\"<updated_taxonomy>\")[1].split(\"</updated_taxonomy>\")[0].strip()\n",
    "                try:\n",
    "                    updated_taxonomy = json.loads(taxonomy_section)\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: could not parse taxonomy JSON:\", e)\n",
    "                    updated_taxonomy = taxonomy  # keep last good taxonomy\n",
    "\n",
    "            return skills_map, updated_taxonomy\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # exponential backoff\n",
    "\n",
    "    print(\"❌ Failed after max retries\")\n",
    "    return {}, taxonomy\n",
    "\n",
    "# -------------------------------\n",
    "# Process in batches\n",
    "# -------------------------------\n",
    "unprocessed = df[df[\"generic_skill\"].isna()].copy()\n",
    "\n",
    "for start in range(0, len(unprocessed), BATCH_SIZE):\n",
    "    batch = unprocessed.iloc[start:start+BATCH_SIZE]\n",
    "    print(f\"Processing {start}–{start+len(batch)-1}...\")\n",
    "\n",
    "    skills_map, taxonomy = classify_tasks(batch, taxonomy)\n",
    "\n",
    "    # Update DataFrame\n",
    "    for task_id, skill in skills_map.items():\n",
    "        df.loc[df[\"TaskID\"] == task_id, \"generic_skill\"] = skill\n",
    "\n",
    "    # Save progress atomically\n",
    "    atomic_write_csv(df, OUTPUT_FILE)\n",
    "    atomic_write_json(taxonomy, TAXONOMY_FILE)\n",
    "\n",
    "print(\"✅ Done! Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2f46f-77ec-4f6f-90e4-6747bed2e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using benchmark tasks, map them to the common taxonomy\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import tempfile\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import openai\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"\"\n",
    "BASE_URL = \"\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "BENCHMARK_FILE = \"biocoder_tasks.txt\"\n",
    "TAXONOMY_FILE = \"../taxonomy_restructured.json\"\n",
    "TAXONOMY_BACKUP_FILE = \"../taxonomy_restructured.json.backup\"\n",
    "OUTPUT_FILE = \"biocoder_tasks_mapping_re.json\"\n",
    "CHECKPOINT_FILE = \"biocoder_tasks_progress_re.json\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "INITIAL_SAMPLE_MIN = 20\n",
    "ALPHA = 0.1\n",
    "DESIRED_COVERAGE = 0.90\n",
    "MAX_CONSEC_NO_NEW = 10\n",
    "SAMPLE_SEED = 42\n",
    "\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, path):\n",
    "    \"\"\"Safely write JSON with temp + replace.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf:\n",
    "        json.dump(data, tf, indent=2)\n",
    "        tmp = tf.name\n",
    "    shutil.move(tmp, path)\n",
    "\n",
    "def chao1(S_obs, freq_values):\n",
    "    \"\"\"Chao1 estimator for total species richness.\"\"\"\n",
    "    if len(freq_values) < 2:\n",
    "        return float('inf')\n",
    "    \n",
    "    f1 = sum(1 for v in freq_values if v == 1)\n",
    "    f2 = sum(1 for v in freq_values if v == 2)\n",
    "    \n",
    "    if f1 == 0:\n",
    "        return S_obs\n",
    "    \n",
    "    if f2 == 0:\n",
    "        return S_obs + f1 * (f1 - 1) / 2 if f1 > 1 else S_obs + f1\n",
    "    \n",
    "    return S_obs + f1 * f1 / (2 * f2)\n",
    "\n",
    "# -------------------------------\n",
    "# Load benchmark tasks\n",
    "# -------------------------------\n",
    "with open(BENCHMARK_FILE, \"r\") as f:\n",
    "    lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "benchmark_tasks = []\n",
    "for line in lines:\n",
    "    if \":\" in line:\n",
    "        task_id, instruction = line.split(\":\", 1)\n",
    "        benchmark_tasks.append({\n",
    "            \"benchmark_task_id\": task_id.strip(),\n",
    "            \"instruction\": instruction.strip()\n",
    "        })\n",
    "\n",
    "# -------------------------------\n",
    "# Load taxonomy\n",
    "# -------------------------------\n",
    "if not os.path.exists(TAXONOMY_BACKUP_FILE) and os.path.exists(TAXONOMY_FILE):\n",
    "    print(f\"[BACKUP] Creating backup: {TAXONOMY_BACKUP_FILE}\")\n",
    "    shutil.copy2(TAXONOMY_FILE, TAXONOMY_BACKUP_FILE)\n",
    "\n",
    "with open(TAXONOMY_FILE, \"r\") as f:\n",
    "    taxonomy = json.load(f)\n",
    "\n",
    "def init_leaf_dict(node):\n",
    "    if isinstance(node, dict):\n",
    "        for k, v in list(node.items()):\n",
    "            if v == {}:\n",
    "                node[k] = {\"tasks\": []}\n",
    "            else:\n",
    "                init_leaf_dict(v)\n",
    "\n",
    "init_leaf_dict(taxonomy[\"root\"])\n",
    "\n",
    "# -------------------------------\n",
    "# State (resume if checkpoint exists)\n",
    "# -------------------------------\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    remaining_indices = state[\"remaining_indices\"]\n",
    "    discovered_leaves = Counter({tuple(k): v for k, v in state[\"discovered_leaves\"].items()})\n",
    "    consec_no_new = state[\"consec_no_new\"]\n",
    "    mapping_log = state[\"mapping_log\"]\n",
    "    taxonomy = state[\"taxonomy\"]\n",
    "    print(f\"[RESUME] Sampled: {len(mapping_log)}, Remaining: {len(remaining_indices)}, Unique leaves: {len(discovered_leaves)}\")\n",
    "else:\n",
    "    random.seed(SAMPLE_SEED)\n",
    "    remaining_indices = list(range(len(benchmark_tasks)))\n",
    "    random.shuffle(remaining_indices)\n",
    "    discovered_leaves = Counter()\n",
    "    consec_no_new = 0\n",
    "    mapping_log = {}\n",
    "    print(f\"[START] Total tasks: {len(benchmark_tasks)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# LLM mapping function\n",
    "# -------------------------------\n",
    "def map_tasks_to_taxonomy(tasks_batch, taxonomy):\n",
    "    tasks_str = \"\\n\".join(\n",
    "        [f\"{i+1}. [{t['benchmark_task_id']}] {t['instruction']}\" for i, t in enumerate(tasks_batch)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert occupational skill mapper.\n",
    "\n",
    "Here is the current taxonomy (JSON):\n",
    "<taxonomy>\n",
    "{json.dumps(taxonomy, indent=2)}\n",
    "</taxonomy>\n",
    "\n",
    "You are given the following benchmark tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Instructions:\n",
    "- For each benchmark task, assign one or more relevant leaves (skills) from the taxonomy. The skills must be directly the function of the task as described by the task description.\n",
    "- Do NOT extrapolate the task description to related skills, but map exactly the skills required to the task.\n",
    "- If there is no direct skill that is related to the task, or if the task is not sufficiently representative of the skill, output \"N/A\" for the task.\n",
    "- Multiple leaves can be assigned per task.\n",
    "- Only use exact existing categories, do NOT invent new ones.\n",
    "- Output must include:\n",
    "<task_mappings>\n",
    "- benchmark_task_id: [ \"root/Category/Subcategory\", ... ]\n",
    "</task_mappings>\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in skill mapping and taxonomy assignment.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content if response.choices else \"\"\n",
    "    print(content)\n",
    "\n",
    "    task_map = {}\n",
    "    if \"<task_mappings>\" in content and \"</task_mappings>\" in content:\n",
    "        mapping_section = content.split(\"<task_mappings>\")[1].split(\"</task_mappings>\")[0].strip()\n",
    "        \n",
    "        # Try to parse the entire mapping section as one JSON-like structure\n",
    "        try:\n",
    "            lines = mapping_section.splitlines()\n",
    "            json_str = \"{\"\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"-\") and \":\" in line:\n",
    "                    parts = line[1:].split(\":\", 1)\n",
    "                    task_id = parts[0].strip()\n",
    "                    value_part = parts[1].strip()\n",
    "                    if json_str != \"{\":\n",
    "                        json_str += \",\"\n",
    "                    json_str += f'\"{task_id}\": {value_part}'\n",
    "                elif line and not line.startswith(\"-\"):\n",
    "                    json_str += line\n",
    "            json_str += \"}\"\n",
    "            \n",
    "            parsed = json.loads(json_str.replace(\"'\", '\"'))\n",
    "            task_map.update(parsed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed bulk parsing, falling back to line-by-line: {e}\")\n",
    "            \n",
    "            current_task_id = None\n",
    "            current_value = \"\"\n",
    "            \n",
    "            for line in mapping_section.splitlines():\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"-\") and \":\" in line:\n",
    "                    if current_task_id:\n",
    "                        try:\n",
    "                            if current_value.upper() == \"N/A\":\n",
    "                                task_map[current_task_id] = []\n",
    "                            else:\n",
    "                                paths = json.loads(current_value.replace(\"'\", '\"'))\n",
    "                                task_map[current_task_id] = paths\n",
    "                        except Exception as ex:\n",
    "                            print(f\"⚠️ Failed parsing task {current_task_id}: {ex}\")\n",
    "                    \n",
    "                    parts = line[1:].split(\":\", 1)\n",
    "                    current_task_id = parts[0].strip()\n",
    "                    current_value = parts[1].strip()\n",
    "                elif line and current_task_id:\n",
    "                    current_value += \" \" + line\n",
    "            \n",
    "            if current_task_id:\n",
    "                try:\n",
    "                    if current_value.upper() == \"N/A\":\n",
    "                        task_map[current_task_id] = []\n",
    "                    else:\n",
    "                        paths = json.loads(current_value.replace(\"'\", '\"'))\n",
    "                        task_map[current_task_id] = paths\n",
    "                except Exception as ex:\n",
    "                    print(f\"⚠️ Failed parsing task {current_task_id}: {ex}\")\n",
    "\n",
    "    return task_map\n",
    "\n",
    "# -------------------------------\n",
    "# Helper to check path exists\n",
    "# -------------------------------\n",
    "def path_exists(taxonomy, path):\n",
    "    try:\n",
    "        node = taxonomy[\"root\"]\n",
    "        for p in path:\n",
    "            node = node[p]\n",
    "        return \"tasks\" in node\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "# -------------------------------\n",
    "# Normalize and clean path\n",
    "# -------------------------------\n",
    "def normalize_path(path_str):\n",
    "    \"\"\"\n",
    "    Normalize path: remove 'root/', strip '/tasks' suffix, handle missing root.\n",
    "    Returns tuple of path components.\n",
    "    \"\"\"\n",
    "    path_str = path_str.strip().strip('/')\n",
    "    \n",
    "    # Remove \"root/\" prefix if present\n",
    "    if path_str.startswith(\"root/\"):\n",
    "        path_str = path_str[5:]\n",
    "    \n",
    "    # Remove \"/tasks\" suffix if present\n",
    "    if path_str.endswith(\"/tasks\"):\n",
    "        path_str = path_str[:-6]\n",
    "    \n",
    "    # Skip empty paths\n",
    "    if not path_str:\n",
    "        return None\n",
    "        \n",
    "    return tuple(path_str.split(\"/\"))\n",
    "\n",
    "# -------------------------------\n",
    "# Sampling loop\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH SAMPLING WITH COVERAGE-BASED STOPPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "while remaining_indices:\n",
    "    # Get batch\n",
    "    batch_indices = [remaining_indices.pop(0) for _ in range(min(BATCH_SIZE, len(remaining_indices)))]\n",
    "    batch_tasks = [benchmark_tasks[i] for i in batch_indices]\n",
    "\n",
    "    print(f\"\\n[BATCH] Processing {len(batch_tasks)} tasks...\")\n",
    "    task_map = map_tasks_to_taxonomy(batch_tasks, taxonomy)\n",
    "\n",
    "    new_leaf_discovered = False\n",
    "    batch_new_leaves = []\n",
    "    \n",
    "    for task_id, paths in task_map.items():\n",
    "        mapping_log[task_id] = paths\n",
    "        \n",
    "        for path_str in paths:\n",
    "            path = normalize_path(path_str)\n",
    "            \n",
    "            if path is None:\n",
    "                print(f\"⚠️ Skipping empty path for task {task_id}\")\n",
    "                continue\n",
    "            \n",
    "            if path_exists(taxonomy, path):\n",
    "                # Navigate to node and add task\n",
    "                node = taxonomy[\"root\"]\n",
    "                for p in path:\n",
    "                    node = node[p]\n",
    "                \n",
    "                if task_id not in node[\"tasks\"]:\n",
    "                    node[\"tasks\"].append(task_id)\n",
    "                \n",
    "                # Track leaf discovery\n",
    "                if path not in discovered_leaves:\n",
    "                    discovered_leaves[path] = 1\n",
    "                    new_leaf_discovered = True\n",
    "                    batch_new_leaves.append(path)\n",
    "                else:\n",
    "                    discovered_leaves[path] += 1\n",
    "            else:\n",
    "                print(f\"⚠️ Invalid path suggested by LLM: {path_str}\")\n",
    "\n",
    "    # Update consecutive no-new counter\n",
    "    if new_leaf_discovered:\n",
    "        consec_no_new = 0\n",
    "        print(f\"✅ Discovered {len(batch_new_leaves)} new leaf/leaves:\")\n",
    "        for leaf in batch_new_leaves:\n",
    "            print(f\"  - {'/'.join(leaf)}\")\n",
    "    else:\n",
    "        consec_no_new += 1\n",
    "        print(f\"✅ No new leaves discovered ({consec_no_new} consecutive batches)\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    n_sampled = len(mapping_log)\n",
    "    S_obs = len(discovered_leaves)\n",
    "    freq_values = list(discovered_leaves.values())\n",
    "    \n",
    "    p_new = (sum(1 for v in freq_values if v == 1) / n_sampled) if n_sampled > 0 else 1\n",
    "    S_chao = chao1(S_obs, freq_values) if n_sampled > 0 else float(\"inf\")\n",
    "    coverage = S_obs / S_chao if S_chao and math.isfinite(S_chao) else 0\n",
    "\n",
    "    print(f\"\\n[STATS] Sampled: {n_sampled}, Unique leaves: {S_obs}, Chao1: {S_chao:.1f}, Coverage: {coverage:.2%}, P(new): {p_new:.3f}\")\n",
    "\n",
    "    # Stopping conditions\n",
    "    stop_reason = None\n",
    "    if n_sampled >= INITIAL_SAMPLE_MIN:\n",
    "        if p_new <= ALPHA:\n",
    "            stop_reason = f\"Low new discovery probability: {p_new:.3f} <= {ALPHA}\"\n",
    "        elif coverage >= DESIRED_COVERAGE and math.isfinite(S_chao):\n",
    "            stop_reason = f\"Desired coverage reached: {coverage:.2%} >= {DESIRED_COVERAGE:.2%}\"\n",
    "        elif consec_no_new >= MAX_CONSEC_NO_NEW:\n",
    "            stop_reason = f\"No new leaves for {consec_no_new} consecutive batches\"\n",
    "    else:\n",
    "        print(f\"[INFO] Continuing to reach minimum sample size ({n_sampled}/{INITIAL_SAMPLE_MIN})\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    atomic_write_json({\n",
    "        \"remaining_indices\": remaining_indices,\n",
    "        \"discovered_leaves\": {str(list(k)): v for k, v in discovered_leaves.items()},\n",
    "        \"consec_no_new\": consec_no_new,\n",
    "        \"mapping_log\": mapping_log,\n",
    "        \"taxonomy\": taxonomy\n",
    "    }, CHECKPOINT_FILE)\n",
    "    \n",
    "    if stop_reason:\n",
    "        print(f\"\\n[STOPPING] {stop_reason}\")\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# Final save\n",
    "# -------------------------------\n",
    "atomic_write_json({\n",
    "    \"taxonomy\": taxonomy,\n",
    "    \"mapping_log\": mapping_log,\n",
    "    \"statistics\": {\n",
    "        \"total_sampled\": len(mapping_log),\n",
    "        \"unique_leaves\": len(discovered_leaves),\n",
    "        \"chao1_estimate\": S_chao if math.isfinite(S_chao) else None,\n",
    "        \"coverage\": coverage\n",
    "    }\n",
    "}, OUTPUT_FILE)\n",
    "\n",
    "atomic_write_json(taxonomy, TAXONOMY_FILE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY:\")\n",
    "print(f\"  Tasks sampled: {len(mapping_log)}/{len(benchmark_tasks)}\")\n",
    "print(f\"  Unique leaves discovered: {len(discovered_leaves)}\")\n",
    "print(f\"  Chao1 estimate: {S_chao:.1f}\")\n",
    "print(f\"  Coverage: {coverage:.2%}\")\n",
    "print(f\"  Mapping saved to: {OUTPUT_FILE}\")\n",
    "print(f\"  Updated taxonomy: {TAXONOMY_FILE}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_FILE}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3de015-8205-4fe0-a474-dccbabbdcfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging jobs and tasks into one taxonomy\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "JOB_SKILLS_FILE = \"../job_tasks_with_skills.csv\"\n",
    "TAXONOMY_FILE = \"taxonomy_with_instructions.json\"\n",
    "TAXONOMY_INFO_FILE = \"../taxonomy_restructure_info.json\"\n",
    "OUTPUT_FILE = \"taxonomy_with_jobs.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def get_node(taxonomy, path_str):\n",
    "    \"\"\"Navigate to a node in taxonomy given a path string.\"\"\"\n",
    "    parts = path_str.split(\"/\")\n",
    "    if parts[0] != \"root\":\n",
    "        return None\n",
    "    \n",
    "    node = taxonomy[\"root\"]\n",
    "    for part in parts[1:]:\n",
    "        if part in node:\n",
    "            node = node[part]\n",
    "        else:\n",
    "            return None\n",
    "    return node\n",
    "\n",
    "# -------------------------------\n",
    "# Load data\n",
    "# -------------------------------\n",
    "print(\"[LOAD] Loading taxonomy and job data...\")\n",
    "\n",
    "taxonomy = load_json(TAXONOMY_FILE)\n",
    "taxonomy_info = load_json(TAXONOMY_INFO_FILE)\n",
    "task_mapping = taxonomy_info.get(\"task_mapping\", {})\n",
    "# Collect jobs by path\n",
    "jobs_by_path = defaultdict(list)\n",
    "\n",
    "print(\"[LOAD] Reading job skills CSV...\")\n",
    "with open(JOB_SKILLS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        task_id = row[\"TaskID\"]\n",
    "        task_text = row[\"Task\"]\n",
    "        occupation = row[\"Occupation (O*NET-SOC Title)\"]\n",
    "        automation = row[\"Automation Desire Rating\"]\n",
    "        job_security = row[\"Job Security Rating\"]\n",
    "        enjoyment = row[\"Enjoyment Rating\"]\n",
    "        generic_skill = row[\"generic_skill\"]\n",
    "        \n",
    "        # Extract path from generic_skill (format: \"Description (Path)\")\n",
    "        if \"(\" in generic_skill and \")\" in generic_skill:\n",
    "            path_part = generic_skill.split(\"(\")[-1].split(\")\")[0]\n",
    "            \n",
    "            # Convert to taxonomy path format\n",
    "            path_components = [p.strip() for p in path_part.split(\">\")]\n",
    "            original_path = \"root/\" + \"/\".join(path_components)\n",
    "            mapped_path = original_path\n",
    "            # Fix duplicate root/ prefix if present\n",
    "            if mapped_path.startswith(\"root/root/\"):\n",
    "                final_path = mapped_path[5:]  # Remove the first \"root/\"\n",
    "            else:\n",
    "                final_path = mapped_path\n",
    "            final_path = task_mapping.get(final_path.strip(), final_path)\n",
    "            \n",
    "            # Add job to the final path\n",
    "            jobs_by_path[final_path].append({\n",
    "                \"job_task_id\": task_id,\n",
    "                \"task\": task_text,\n",
    "                \"occupation\": occupation,\n",
    "                \"automation_desire\": automation,\n",
    "                \"job_security\": job_security,\n",
    "                \"enjoyment\": enjoyment\n",
    "            })\n",
    "\n",
    "print(f\"[LOAD] Found {len(jobs_by_path)} unique paths with jobs\")\n",
    "print(f\"[LOAD] Total job tasks: {sum(len(jobs) for jobs in jobs_by_path.values())}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Add jobs to taxonomy\n",
    "# -------------------------------\n",
    "print(\"\\n[PROCESS] Adding jobs to taxonomy nodes...\")\n",
    "\n",
    "jobs_added = 0\n",
    "paths_with_jobs = 0\n",
    "paths_not_found = []\n",
    "\n",
    "for path, jobs in jobs_by_path.items():\n",
    "    node = get_node(taxonomy, path)\n",
    "    if node is not None:\n",
    "        if \"jobs\" not in node:\n",
    "            node[\"jobs\"] = []\n",
    "        \n",
    "        # Add each job if not already present\n",
    "        for job in jobs:\n",
    "            # Check if job already exists (by job_task_id)\n",
    "            existing_ids = [j[\"job_task_id\"] for j in node[\"jobs\"] if isinstance(j, dict)]\n",
    "            if job[\"job_task_id\"] not in existing_ids:\n",
    "                node[\"jobs\"].append(job)\n",
    "                jobs_added += 1\n",
    "        \n",
    "        paths_with_jobs += 1\n",
    "    else:\n",
    "        paths_not_found.append(path)\n",
    "        print(f\"⚠️ Path not found in taxonomy: {path}\")\n",
    "\n",
    "print(f\"\\n[SUMMARY]\")\n",
    "print(f\"  Paths with jobs added: {paths_with_jobs}\")\n",
    "print(f\"  Total jobs added: {jobs_added}\")\n",
    "print(f\"  Paths not found: {len(paths_not_found)}\")\n",
    "\n",
    "if paths_not_found:\n",
    "    print(f\"\\n[WARNING] The following paths were not found:\")\n",
    "    for path in paths_not_found[:10]:  # Show first 10\n",
    "        print(f\"  - {path}\")\n",
    "    if len(paths_not_found) > 10:\n",
    "        print(f\"  ... and {len(paths_not_found) - 10} more\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save updated taxonomy\n",
    "# -------------------------------\n",
    "save_json(taxonomy, OUTPUT_FILE)\n",
    "print(f\"\\n✅ Updated taxonomy saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Generate statistics\n",
    "# -------------------------------\n",
    "def count_jobs_in_taxonomy(node):\n",
    "    \"\"\"Recursively count jobs in taxonomy.\"\"\"\n",
    "    count = 0\n",
    "    if isinstance(node, dict):\n",
    "        if \"jobs\" in node:\n",
    "            count += len([j for j in node[\"jobs\"] if isinstance(j, dict)])\n",
    "        for k, v in node.items():\n",
    "            if k not in [\"tasks\", \"jobs\"]:\n",
    "                count += count_jobs_in_taxonomy(v)\n",
    "    return count\n",
    "\n",
    "total_jobs = count_jobs_in_taxonomy(taxonomy[\"root\"])\n",
    "print(f\"\\n[STATS] Total jobs in taxonomy: {total_jobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c0f5d-e245-4f13-8ef5-cb0731fdc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similarity between tasks and jobs on same taxonomy leaves\n",
    "\"\"\"\n",
    "Integrate new benchmark tasks into existing taxonomy with job similarities.\n",
    "This will:\n",
    "1. Add new benchmark tasks to taxonomy_with_similarity.json\n",
    "2. Recalculate similarities only for affected leaves\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "import openai\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"sk-52xpth3jJx48lbKk5dgStg\"\n",
    "BASE_URL = \"https://ai-gateway.andrew.cmu.edu/\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "TAXONOMY_FILE = \"taxonomy_with_similarity.json\"  # Start from existing file with similarities\n",
    "OUTPUT_FILE = \"taxonomy_with_similarity_updated.json\"\n",
    "CHECKPOINT_FILE = \"integration_progress.json\"\n",
    "\n",
    "TASK_BATCH_SIZE = 10\n",
    "\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, path):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf:\n",
    "        json.dump(data, tf, indent=2)\n",
    "        tmp = tf.name\n",
    "    shutil.move(tmp, path)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_node(taxonomy, path_list):\n",
    "    node = taxonomy\n",
    "    for p in path_list:\n",
    "        if p not in node:\n",
    "            return None\n",
    "        node = node[p]\n",
    "    return node\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Merge new benchmark tasks\n",
    "# -------------------------------\n",
    "def merge_new_benchmarks(taxonomy):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: MERGING NEW BENCHMARK TASKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    affected_leaves = set()\n",
    "    \n",
    "    task_files = sorted(glob(\"*_tasks.txt\"))\n",
    "    print(f\"\\nFound {len(task_files)} benchmark file(s)\")\n",
    "    \n",
    "    for task_file in task_files:\n",
    "        base = task_file.replace(\"_tasks.txt\", \"\")\n",
    "        mapping_file = f\"{base}_tasks_mapping_re.json\"\n",
    "        \n",
    "        if not os.path.exists(mapping_file):\n",
    "            print(f\"Skipping {base}: no mapping file\")\n",
    "            continue\n",
    "        \n",
    "        benchmark = base\n",
    "        print(f\"\\nProcessing: {benchmark}\")\n",
    "        \n",
    "        # Load tasks\n",
    "        tasks = {}\n",
    "        with open(task_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \":\" in line:\n",
    "                    tid, inst = line.split(\":\", 1)\n",
    "                    tasks[tid.strip()] = inst.strip()\n",
    "        \n",
    "        print(f\"  Loaded {len(tasks)} tasks\")\n",
    "        \n",
    "        # Load mapping\n",
    "        mapping = load_json(mapping_file)\n",
    "        mapping_tax = mapping.get(\"taxonomy\", {}).get(\"root\", {})\n",
    "        \n",
    "        # Process mapping\n",
    "        added = 0\n",
    "        \n",
    "        def traverse(node, path):\n",
    "            nonlocal added\n",
    "            if not isinstance(node, dict):\n",
    "                return\n",
    "            \n",
    "            if \"tasks\" in node:\n",
    "                task_ids = [t for t in node[\"tasks\"] if isinstance(t, str) and t in tasks]\n",
    "                if task_ids:\n",
    "                    master = get_node(taxonomy, path)\n",
    "                    if master:\n",
    "                        # Initialize\n",
    "                        if \"tasks\" not in master:\n",
    "                            master[\"tasks\"] = []\n",
    "                        \n",
    "                        # Clean string IDs\n",
    "                        master[\"tasks\"] = [t for t in master[\"tasks\"] if isinstance(t, dict)]\n",
    "                        \n",
    "                        # Get existing IDs\n",
    "                        existing = {t.get(\"task_id\") for t in master[\"tasks\"]}\n",
    "                        \n",
    "                        # Add new tasks\n",
    "                        for tid in task_ids:\n",
    "                            if tid not in existing:\n",
    "                                master[\"tasks\"].append({\n",
    "                                    \"benchmark\": benchmark,\n",
    "                                    \"task_id\": tid,\n",
    "                                    \"instruction\": tasks[tid],\n",
    "                                    \"job_similarities\": []  # Will be filled later\n",
    "                                })\n",
    "                                added += 1\n",
    "                                affected_leaves.add(tuple(path))\n",
    "            \n",
    "            for k, v in node.items():\n",
    "                if k != \"tasks\":\n",
    "                    traverse(v, path + [k])\n",
    "        \n",
    "        traverse(mapping_tax, [\"root\"])\n",
    "        print(f\"  Added {added} new task objects\")\n",
    "    \n",
    "    print(f\"\\n✓ Total affected leaves: {len(affected_leaves)}\")\n",
    "    return affected_leaves\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Recalculate similarities for affected leaves\n",
    "# -------------------------------\n",
    "def calculate_similarity_batch(benchmark_tasks, job_tasks, skill_path):\n",
    "    tasks_str = \"\\n\".join([\n",
    "        f\"{i+1}. [ID: {t['task_id']}] {t['instruction']}\"\n",
    "        for i, t in enumerate(benchmark_tasks)\n",
    "    ])\n",
    "    \n",
    "    jobs_str = \"\\n\".join([\n",
    "        f\"{i+1}. [ID: {j['job_task_id']}] {j['task']} (Occupation: {j['occupation']})\"\n",
    "        for i, j in enumerate(job_tasks)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert at comparing and rating task similarity in occupational contexts.\n",
    "\n",
    "You are analyzing tasks within the skill category: {skill_path}\n",
    "\n",
    "Here are BENCHMARK TASKS (from automated task benchmarks):\n",
    "{tasks_str}\n",
    "\n",
    "Here are JOB TASKS (from real occupations):\n",
    "{jobs_str}\n",
    "\n",
    "For each BENCHMARK TASK, evaluate its similarity to each JOB TASK on a scale of 0-10:\n",
    "- 0: Completely unrelated\n",
    "- 1-3: Minimal relation (shares very broad concepts only)\n",
    "- 4-6: Moderate relation (overlapping skills but different contexts/goals)\n",
    "- 7-9: Strong relation (similar skills and contexts, different specifics)\n",
    "- 10: Nearly identical (same skills, same context, same goal)\n",
    "\n",
    "Consider:\n",
    "- What specific skills are required?\n",
    "- How well is the benchmark task representative of the skills required for the job?\n",
    "- How close in terms of complexity is the benchmark task to the job task?\n",
    "- What is the context/domain?\n",
    "- What is the end goal/outcome?\n",
    "- How transferable are the skills?\n",
    "\n",
    "Output format:\n",
    "<similarities>\n",
    "benchmark_task_id: [\n",
    "  {{\"job_task_id\": \"X\", \"score\": Y, \"reasoning\": \"brief explanation\"}},\n",
    "  ...\n",
    "]\n",
    "</similarities>\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in occupational task analysis and skill matching.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content if response.choices else \"\"\n",
    "    \n",
    "    # Parse response\n",
    "    similarity_map = {}\n",
    "    if \"<similarities>\" in content and \"</similarities>\" in content:\n",
    "        sim_section = content.split(\"<similarities>\")[1].split(\"</similarities>\")[0].strip()\n",
    "        \n",
    "        current_task_id = None\n",
    "        current_json = \"\"\n",
    "        \n",
    "        for line in sim_section.splitlines():\n",
    "            line = line.strip()\n",
    "            \n",
    "            if \":\" in line and line.endswith(\"[\"):\n",
    "                if current_task_id and current_json:\n",
    "                    try:\n",
    "                        similarity_map[current_task_id] = json.loads(current_json)\n",
    "                    except:\n",
    "                        pass\n",
    "                current_task_id = line.split(\":\")[0].strip()\n",
    "                current_json = \"[\"\n",
    "            elif line.startswith(\"]\"):\n",
    "                current_json += \"]\"\n",
    "                if current_task_id:\n",
    "                    try:\n",
    "                        similarity_map[current_task_id] = json.loads(current_json)\n",
    "                    except:\n",
    "                        pass\n",
    "                current_task_id = None\n",
    "                current_json = \"\"\n",
    "            elif current_task_id:\n",
    "                current_json += line\n",
    "        \n",
    "        if current_task_id and current_json:\n",
    "            try:\n",
    "                if not current_json.endswith(\"]\"):\n",
    "                    current_json += \"]\"\n",
    "                similarity_map[current_task_id] = json.loads(current_json)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return similarity_map\n",
    "\n",
    "def recalculate_similarities(taxonomy, affected_leaves):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: RECALCULATING SIMILARITIES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        checkpoint = load_json(CHECKPOINT_FILE)\n",
    "        processed = set(checkpoint.get(\"processed_leaves\", []))\n",
    "    else:\n",
    "        processed = set()\n",
    "    \n",
    "    leaves_to_process = [\n",
    "        leaf for leaf in affected_leaves \n",
    "        if \"/\".join(leaf) not in processed\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTotal affected leaves: {len(affected_leaves)}\")\n",
    "    print(f\"Already processed: {len(processed)}\")\n",
    "    print(f\"Remaining: {len(leaves_to_process)}\")\n",
    "    \n",
    "    total_comparisons = 0\n",
    "    \n",
    "    for idx, leaf_path in enumerate(leaves_to_process):\n",
    "        leaf_str = \"/\".join(leaf_path)\n",
    "        print(f\"\\n[{idx+1}/{len(leaves_to_process)}] {leaf_str}\")\n",
    "        \n",
    "        node = get_node(taxonomy, list(leaf_path))\n",
    "        if not node:\n",
    "            continue\n",
    "        \n",
    "        tasks = [t for t in node.get(\"tasks\", []) if isinstance(t, dict)]\n",
    "        jobs = [j for j in node.get(\"jobs\", []) if isinstance(j, dict)]\n",
    "        \n",
    "        # Find tasks without similarities\n",
    "        tasks_needing_sim = [t for t in tasks if not t.get(\"job_similarities\")]\n",
    "        \n",
    "        print(f\"  Tasks: {len(tasks)}, Jobs: {len(jobs)}, Need similarity: {len(tasks_needing_sim)}\")\n",
    "        \n",
    "        if not tasks_needing_sim or not jobs:\n",
    "            processed.add(leaf_str)\n",
    "            continue\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, len(tasks_needing_sim), TASK_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + TASK_BATCH_SIZE, len(tasks_needing_sim))\n",
    "            task_batch = tasks_needing_sim[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"  Processing tasks {batch_start+1}-{batch_end}...\")\n",
    "            \n",
    "            try:\n",
    "                similarities = calculate_similarity_batch(task_batch, jobs, leaf_str)\n",
    "                \n",
    "                for task in task_batch:\n",
    "                    task_id = task[\"task_id\"]\n",
    "                    if task_id in similarities:\n",
    "                        task[\"job_similarities\"] = similarities[task_id]\n",
    "                    else:\n",
    "                        task[\"job_similarities\"] = []\n",
    "                \n",
    "                total_comparisons += len(task_batch) * len(jobs)\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Error: {e}\")\n",
    "        \n",
    "        processed.add(leaf_str)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        atomic_write_json({\n",
    "            \"processed_leaves\": list(processed),\n",
    "            \"taxonomy\": taxonomy\n",
    "        }, CHECKPOINT_FILE)\n",
    "    \n",
    "    print(f\"\\n✓ Completed {total_comparisons} comparisons\")\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"INTEGRATING NEW BENCHMARK TASKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load existing taxonomy with similarities\n",
    "    print(f\"\\nLoading: {TAXONOMY_FILE}\")\n",
    "    taxonomy = load_json(TAXONOMY_FILE)\n",
    "    \n",
    "    # Step 1: Merge new tasks\n",
    "    affected_leaves = merge_new_benchmarks(taxonomy)\n",
    "    \n",
    "    # Step 2: Recalculate similarities only for affected leaves\n",
    "    if affected_leaves:\n",
    "        recalculate_similarities(taxonomy, affected_leaves)\n",
    "    else:\n",
    "        print(\"\\nNo new tasks added, nothing to recalculate\")\n",
    "    \n",
    "    # Save final result\n",
    "    atomic_write_json(taxonomy, OUTPUT_FILE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPLETE\")\n",
    "    print(f\"Output: {OUTPUT_FILE}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1766bdb2-e28a-4cc3-8ef9-79d55e35fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "JOB COVERAGE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Similarity threshold: 7\n",
      "Loading: taxonomy_with_similarity_updated.json\n",
      "\n",
      "Total leaves: 322\n",
      "Leaves with jobs: 275\n",
      "\n",
      "================================================================================\n",
      "METRIC 1: COVERAGE\n",
      "================================================================================\n",
      "Leaves with jobs that also have tasks: 187/275\n",
      "Coverage: 68.00%\n",
      "\n",
      "================================================================================\n",
      "METRIC 2: COVERAGE QUALITY\n",
      "================================================================================\n",
      "Total jobs analyzed: 673\n",
      "Jobs with high-quality tasks (>=7): 444\n",
      "Quality coverage: 65.97%\n",
      "\n",
      "================================================================================\n",
      "DISTRIBUTION: Tasks per Job\n",
      "================================================================================\n",
      "0 tasks: 229 jobs (34.0%)\n",
      "1 tasks: 128 jobs (19.0%)\n",
      "2 tasks: 80 jobs (11.9%)\n",
      "3 tasks: 46 jobs (6.8%)\n",
      "4 tasks: 33 jobs (4.9%)\n",
      "5 tasks: 24 jobs (3.6%)\n",
      "6 tasks: 11 jobs (1.6%)\n",
      "7 tasks: 17 jobs (2.5%)\n",
      "8 tasks: 15 jobs (2.2%)\n",
      "9 tasks: 8 jobs (1.2%)\n",
      "10 tasks: 4 jobs (0.6%)\n",
      "11 tasks: 4 jobs (0.6%)\n",
      "12 tasks: 5 jobs (0.7%)\n",
      "13 tasks: 7 jobs (1.0%)\n",
      "14 tasks: 4 jobs (0.6%)\n",
      "15 tasks: 4 jobs (0.6%)\n",
      "16 tasks: 1 jobs (0.1%)\n",
      "17 tasks: 2 jobs (0.3%)\n",
      "18 tasks: 2 jobs (0.3%)\n",
      "20 tasks: 7 jobs (1.0%)\n",
      "21 tasks: 2 jobs (0.3%)\n",
      "22 tasks: 1 jobs (0.1%)\n",
      "23 tasks: 2 jobs (0.3%)\n",
      "24 tasks: 1 jobs (0.1%)\n",
      "25 tasks: 1 jobs (0.1%)\n",
      "26 tasks: 1 jobs (0.1%)\n",
      "27 tasks: 1 jobs (0.1%)\n",
      "28 tasks: 2 jobs (0.3%)\n",
      "29 tasks: 1 jobs (0.1%)\n",
      "30 tasks: 3 jobs (0.4%)\n",
      "32 tasks: 2 jobs (0.3%)\n",
      "33 tasks: 1 jobs (0.1%)\n",
      "34 tasks: 2 jobs (0.3%)\n",
      "36 tasks: 1 jobs (0.1%)\n",
      "38 tasks: 2 jobs (0.3%)\n",
      "41 tasks: 1 jobs (0.1%)\n",
      "43 tasks: 1 jobs (0.1%)\n",
      "45 tasks: 1 jobs (0.1%)\n",
      "48 tasks: 1 jobs (0.1%)\n",
      "50 tasks: 1 jobs (0.1%)\n",
      "51 tasks: 1 jobs (0.1%)\n",
      "53 tasks: 1 jobs (0.1%)\n",
      "54 tasks: 1 jobs (0.1%)\n",
      "57 tasks: 1 jobs (0.1%)\n",
      "58 tasks: 1 jobs (0.1%)\n",
      "63 tasks: 1 jobs (0.1%)\n",
      "74 tasks: 1 jobs (0.1%)\n",
      "93 tasks: 1 jobs (0.1%)\n",
      "102 tasks: 1 jobs (0.1%)\n",
      "106 tasks: 1 jobs (0.1%)\n",
      "154 tasks: 1 jobs (0.1%)\n",
      "219 tasks: 1 jobs (0.1%)\n",
      "301 tasks: 1 jobs (0.1%)\n",
      "321 tasks: 1 jobs (0.1%)\n",
      "\n",
      "================================================================================\n",
      "JOBS WITH NO HIGH-QUALITY TASKS: 229\n",
      "================================================================================\n",
      "\n",
      "[253] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Order Fulfillment\n",
      "  Task: Contact suppliers to verify shipment details....\n",
      "\n",
      "[770] Securities, Commodities, and Financial Services Sales Agents\n",
      "  Path: root/Operations & Scheduling/Operations Core/Order Fulfillment\n",
      "  Task: Complete sales order tickets and submit for processing of client-requested transactions....\n",
      "\n",
      "[68] Medical Secretaries and Administrative Assistants\n",
      "  Path: root/Operations & Scheduling/Operations Core/Inventory Management\n",
      "  Task: Perform various clerical or administrative functions, such as ordering and maintaining an inventory ...\n",
      "\n",
      "[487] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Inventory Management\n",
      "  Task: Requisition and maintain inventories of materials or supplies necessary to meet production demands....\n",
      "\n",
      "[51] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Monitor meters, flow gauges, or other real-time data to ensure proper operation of biofuels producti...\n",
      "\n",
      "[132] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Adjust temperature, pressure, vacuum, level, flow rate, or transfer of biofuels to maintain processe...\n",
      "\n",
      "[158] Petroleum Engineers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Monitor production rates, and plan rework processes to improve production....\n",
      "\n",
      "[294] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Compile information, such as production rates and progress, materials inventories, materials used, o...\n",
      "\n",
      "[634] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Review logs, datasheets, or reports to ensure adequate production levels or to identify abnormalitie...\n",
      "\n",
      "[739] Editors\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Oversee publication production, including artwork, layout, computer typesetting, and printing, ensur...\n",
      "\n",
      "... and 219 more\n",
      "\n",
      "================================================================================\n",
      "JOBS WITH BEST COVERAGE (Top 10)\n",
      "================================================================================\n",
      "\n",
      "[655] Secretaries and Administrative Assistants, Except Legal, Medical, and Executive (321 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Conduct searches to find needed information, using such sources as the Internet....\n",
      "\n",
      "[725] Librarians and Media Collections Specialists (301 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Search standard reference materials, including online sources and the Internet, to answer patrons' r...\n",
      "\n",
      "[247] Social Science Research Assistants (219 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Conduct internet-based and library research....\n",
      "\n",
      "[826] Librarians and Media Collections Specialists (154 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Locate unusual or unique information in response to specific requests....\n",
      "\n",
      "[28] News Analysts, Reporters, and Journalists (106 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Check reference materials, such as books, news files, or public records, to obtain relevant facts....\n",
      "\n",
      "[373] Medical Secretaries and Administrative Assistants (102 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Document Production & Distribution\n",
      "  Task: Operate office equipment, such as voice mail messaging systems, and use word processing, spreadsheet...\n",
      "\n",
      "[814] Librarians and Media Collections Specialists (93 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Compile lists of books, periodicals, articles, and audio-visual materials on particular subjects....\n",
      "\n",
      "[688] Computer Programmers (74 tasks)\n",
      "  Path: root/Technology & Engineering/Software Development/Software Maintenance and Enhancement\n",
      "  Task: Perform or direct revision, repair, or expansion of existing programs to increase operating efficien...\n",
      "\n",
      "[362] Business Intelligence Analysts (63 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Collect business intelligence data from available industry reports, public information, field report...\n",
      "\n",
      "[439] Computer Programmers (58 tasks)\n",
      "  Path: root/Technology & Engineering/Software QA & Testing/Defect Remediation\n",
      "  Task: Correct errors by making appropriate changes and rechecking the program to ensure that the desired r...\n",
      "\n",
      "================================================================================\n",
      "METRIC 3: BENCHMARK COVERAGE (Greedy Set Cover)\n",
      "================================================================================\n",
      "\n",
      "Greedy Benchmark Selection (by marginal job coverage):\n",
      "\n",
      "Rank   Benchmark                 New Jobs     Total Jobs   Cumulative   Coverage %\n",
      "-------------------------------------------------------------------------------------\n",
      "1      gdpval                    255          255          255          37.9%\n",
      "2      ColBench                  52           100          307          45.6%\n",
      "3      flowbench                 37           53           344          51.1%\n",
      "4      toolbench                 26           90           370          55.0%\n",
      "5      TAC                       20           94           390          57.9%\n",
      "6      officebench               10           67           400          59.4%\n",
      "7      terminalbench             9            44           409          60.8%\n",
      "8      taskbench                 8            40           417          62.0%\n",
      "9      swebenchpro               8            16           425          63.2%\n",
      "10     galileo                   5            54           430          63.9%\n",
      "11     labbench                  3            8            433          64.3%\n",
      "12     crmarena                  2            14           435          64.6%\n",
      "13     osworld                   2            46           437          64.9%\n",
      "14     algotune                  2            10           439          65.2%\n",
      "15     webarena                  1            4            440          65.4%\n",
      "16     mind2web                  1            27           441          65.5%\n",
      "17     mldev                     1            4            442          65.7%\n",
      "18     bird                      1            13           443          65.8%\n",
      "19     mlebench                  1            10           444          66.0%\n",
      "20     swebenchm                 1            6            445          66.1%\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED BENCHMARK SELECTION\n",
      "================================================================================\n",
      "\n",
      "To cover 80% of jobs (538/673), use these benchmarks in order:\n",
      "1. gdpval (+255 new jobs)\n",
      "2. ColBench (+52 new jobs)\n",
      "3. flowbench (+37 new jobs)\n",
      "4. toolbench (+26 new jobs)\n",
      "5. TAC (+20 new jobs)\n",
      "6. officebench (+10 new jobs)\n",
      "7. terminalbench (+9 new jobs)\n",
      "8. taskbench (+8 new jobs)\n",
      "9. swebenchpro (+8 new jobs)\n",
      "10. galileo (+5 new jobs)\n",
      "11. labbench (+3 new jobs)\n",
      "12. crmarena (+2 new jobs)\n",
      "13. osworld (+2 new jobs)\n",
      "14. algotune (+2 new jobs)\n",
      "15. webarena (+1 new jobs)\n",
      "16. mind2web (+1 new jobs)\n",
      "17. mldev (+1 new jobs)\n",
      "18. bird (+1 new jobs)\n",
      "19. mlebench (+1 new jobs)\n",
      "20. swebenchm (+1 new jobs)\n",
      "\n",
      "Final coverage: 445/673 jobs (66.1%)\n",
      "\n",
      "================================================================================\n",
      "Results saved to: job_coverage_metrics.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate job coverage metrics from taxonomy with similarities.\n",
    "Metrics:\n",
    "1. Coverage: % of leaves with jobs that also have tasks\n",
    "2. Coverage Quality: For each job, count tasks with similarity >= threshold\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "TAXONOMY_FILE = \"taxonomy_with_similarity_updated.json\"\n",
    "OUTPUT_FILE = \"job_coverage_metrics.json\"\n",
    "SIMILARITY_THRESHOLD = 7  # Configurable: 5, 6, 7, etc.\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def find_all_leaves(node, path=[]):\n",
    "    \"\"\"Recursively find all leaf nodes.\"\"\"\n",
    "    leaves = []\n",
    "    if isinstance(node, dict):\n",
    "        has_tasks_or_jobs = \"tasks\" in node or \"jobs\" in node\n",
    "        is_leaf = has_tasks_or_jobs\n",
    "        \n",
    "        if is_leaf:\n",
    "            leaves.append({\n",
    "                \"path\": path,\n",
    "                \"tasks\": node.get(\"tasks\", []),\n",
    "                \"jobs\": node.get(\"jobs\", [])\n",
    "            })\n",
    "        \n",
    "        for k, v in node.items():\n",
    "            if k not in [\"tasks\", \"jobs\"]:\n",
    "                leaves.extend(find_all_leaves(v, path + [k]))\n",
    "    \n",
    "    return leaves\n",
    "\n",
    "# -------------------------------\n",
    "# Main analysis\n",
    "# -------------------------------\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"JOB COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSimilarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "    \n",
    "    # Load taxonomy\n",
    "    print(f\"Loading: {TAXONOMY_FILE}\")\n",
    "    taxonomy = load_json(TAXONOMY_FILE)\n",
    "    \n",
    "    # Find all leaves\n",
    "    all_leaves = find_all_leaves(taxonomy[\"root\"], [\"root\"])\n",
    "    print(f\"\\nTotal leaves: {len(all_leaves)}\")\n",
    "    \n",
    "    # Filter leaves with jobs\n",
    "    leaves_with_jobs = [\n",
    "        leaf for leaf in all_leaves \n",
    "        if len([j for j in leaf[\"jobs\"] if isinstance(j, dict)]) > 0\n",
    "    ]\n",
    "    print(f\"Leaves with jobs: {len(leaves_with_jobs)}\")\n",
    "    \n",
    "    # Calculate coverage: leaves with both jobs and tasks\n",
    "    leaves_with_both = [\n",
    "        leaf for leaf in leaves_with_jobs\n",
    "        if len([t for t in leaf[\"tasks\"] if isinstance(t, dict)]) > 0\n",
    "    ]\n",
    "    \n",
    "    coverage = len(leaves_with_both) / len(leaves_with_jobs) if leaves_with_jobs else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 1: COVERAGE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Leaves with jobs that also have tasks: {len(leaves_with_both)}/{len(leaves_with_jobs)}\")\n",
    "    print(f\"Coverage: {coverage:.2%}\")\n",
    "    \n",
    "    # Calculate coverage quality: job-level analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 2: COVERAGE QUALITY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    job_coverage_details = []\n",
    "    total_jobs = 0\n",
    "    jobs_with_high_quality_tasks = 0\n",
    "    \n",
    "    for leaf in leaves_with_both:\n",
    "        path_str = \"/\".join(leaf[\"path\"])\n",
    "        \n",
    "        jobs = [j for j in leaf[\"jobs\"] if isinstance(j, dict)]\n",
    "        tasks = [t for t in leaf[\"tasks\"] if isinstance(t, dict)]\n",
    "        \n",
    "        for job in jobs:\n",
    "            total_jobs += 1\n",
    "            job_id = job[\"job_task_id\"]\n",
    "            \n",
    "            # Count tasks with high similarity to this job\n",
    "            high_sim_tasks = []\n",
    "            \n",
    "            for task in tasks:\n",
    "                similarities = task.get(\"job_similarities\", [])\n",
    "                for sim in similarities:\n",
    "                    if sim.get(\"job_task_id\") == job_id and sim.get(\"score\", 0) >= SIMILARITY_THRESHOLD:\n",
    "                        high_sim_tasks.append({\n",
    "                            \"task_id\": task[\"task_id\"],\n",
    "                            \"benchmark\": task[\"benchmark\"],\n",
    "                            \"score\": sim[\"score\"],\n",
    "                            \"reasoning\": sim.get(\"reasoning\", \"\")\n",
    "                        })\n",
    "            \n",
    "            if high_sim_tasks:\n",
    "                jobs_with_high_quality_tasks += 1\n",
    "            \n",
    "            job_coverage_details.append({\n",
    "                \"job_task_id\": job_id,\n",
    "                \"occupation\": job[\"occupation\"],\n",
    "                \"job_task\": job[\"task\"],\n",
    "                \"skill_path\": path_str,\n",
    "                \"num_high_quality_tasks\": len(high_sim_tasks),\n",
    "                \"high_quality_tasks\": high_sim_tasks\n",
    "            })\n",
    "    \n",
    "    quality_coverage = jobs_with_high_quality_tasks / total_jobs if total_jobs else 0\n",
    "    \n",
    "    print(f\"Total jobs analyzed: {total_jobs}\")\n",
    "    print(f\"Jobs with high-quality tasks (>={SIMILARITY_THRESHOLD}): {jobs_with_high_quality_tasks}\")\n",
    "    print(f\"Quality coverage: {quality_coverage:.2%}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    task_counts = [jc[\"num_high_quality_tasks\"] for jc in job_coverage_details]\n",
    "    task_count_dist = defaultdict(int)\n",
    "    for count in task_counts:\n",
    "        task_count_dist[count] += 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DISTRIBUTION: Tasks per Job\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for count in sorted(task_count_dist.keys()):\n",
    "        pct = task_count_dist[count] / total_jobs * 100 if total_jobs else 0\n",
    "        print(f\"{count} tasks: {task_count_dist[count]} jobs ({pct:.1f}%)\")\n",
    "    \n",
    "    # Jobs with no coverage\n",
    "    jobs_no_coverage = [jc for jc in job_coverage_details if jc[\"num_high_quality_tasks\"] == 0]\n",
    "    \n",
    "    if jobs_no_coverage:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"JOBS WITH NO HIGH-QUALITY TASKS: {len(jobs_no_coverage)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for jc in jobs_no_coverage[:10]:  # Show first 10\n",
    "            print(f\"\\n[{jc['job_task_id']}] {jc['occupation']}\")\n",
    "            print(f\"  Path: {jc['skill_path']}\")\n",
    "            print(f\"  Task: {jc['job_task'][:100]}...\")\n",
    "        if len(jobs_no_coverage) > 10:\n",
    "            print(f\"\\n... and {len(jobs_no_coverage) - 10} more\")\n",
    "    \n",
    "    # Jobs with best coverage\n",
    "    jobs_best_coverage = sorted(job_coverage_details, key=lambda x: x[\"num_high_quality_tasks\"], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"JOBS WITH BEST COVERAGE (Top 10)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for jc in jobs_best_coverage:\n",
    "        print(f\"\\n[{jc['job_task_id']}] {jc['occupation']} ({jc['num_high_quality_tasks']} tasks)\")\n",
    "        print(f\"  Path: {jc['skill_path']}\")\n",
    "        print(f\"  Task: {jc['job_task'][:100]}...\")\n",
    "    \n",
    "    # Benchmark coverage analysis - GREEDY SET COVER\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 3: BENCHMARK COVERAGE (Greedy Set Cover)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Build mapping: benchmark -> set of jobs it covers well\n",
    "    benchmark_to_jobs = defaultdict(set)\n",
    "    \n",
    "    for leaf in leaves_with_both:\n",
    "        jobs = [j for j in leaf[\"jobs\"] if isinstance(j, dict)]\n",
    "        tasks = [t for t in leaf[\"tasks\"] if isinstance(t, dict)]\n",
    "        \n",
    "        for task in tasks:\n",
    "            benchmark = task.get(\"benchmark\", \"unknown\")\n",
    "            similarities = task.get(\"job_similarities\", [])\n",
    "            \n",
    "            for sim in similarities:\n",
    "                if sim.get(\"score\", 0) >= SIMILARITY_THRESHOLD:\n",
    "                    job_id = sim.get(\"job_task_id\")\n",
    "                    benchmark_to_jobs[benchmark].add(job_id)\n",
    "    \n",
    "    # Greedy set cover: pick benchmark covering most uncovered jobs\n",
    "    covered_jobs = set()\n",
    "    benchmark_ranking = []\n",
    "    remaining_benchmarks = dict(benchmark_to_jobs)\n",
    "    \n",
    "    while remaining_benchmarks:\n",
    "        # Find benchmark that covers most NEW jobs\n",
    "        best_benchmark = None\n",
    "        best_new_coverage = set()\n",
    "        \n",
    "        for bench, jobs_covered in remaining_benchmarks.items():\n",
    "            new_jobs = jobs_covered - covered_jobs\n",
    "            if len(new_jobs) > len(best_new_coverage):\n",
    "                best_benchmark = bench\n",
    "                best_new_coverage = new_jobs\n",
    "        \n",
    "        if not best_benchmark or len(best_new_coverage) == 0:\n",
    "            break\n",
    "        \n",
    "        # Add to ranking\n",
    "        benchmark_ranking.append({\n",
    "            \"rank\": len(benchmark_ranking) + 1,\n",
    "            \"benchmark\": best_benchmark,\n",
    "            \"new_jobs_covered\": len(best_new_coverage),\n",
    "            \"total_jobs_covered\": len(remaining_benchmarks[best_benchmark]),\n",
    "            \"cumulative_coverage\": len(covered_jobs) + len(best_new_coverage)\n",
    "        })\n",
    "        \n",
    "        # Update covered jobs\n",
    "        covered_jobs.update(best_new_coverage)\n",
    "        del remaining_benchmarks[best_benchmark]\n",
    "    \n",
    "    print(f\"\\nGreedy Benchmark Selection (by marginal job coverage):\\n\")\n",
    "    print(f\"{'Rank':<6} {'Benchmark':<25} {'New Jobs':<12} {'Total Jobs':<12} {'Cumulative':<12} {'Coverage %'}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for br in benchmark_ranking:\n",
    "        coverage_pct = br['cumulative_coverage'] / total_jobs * 100 if total_jobs else 0\n",
    "        print(f\"{br['rank']:<6} {br['benchmark']:<25} {br['new_jobs_covered']:<12} {br['total_jobs_covered']:<12} {br['cumulative_coverage']:<12} {coverage_pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RECOMMENDED BENCHMARK SELECTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find minimum set covering target percentage (e.g., 80%)\n",
    "    target_coverage = 0.80\n",
    "    target_jobs = int(target_coverage * total_jobs)\n",
    "    \n",
    "    recommended = []\n",
    "    cumulative = 0\n",
    "    for br in benchmark_ranking:\n",
    "        recommended.append(br['benchmark'])\n",
    "        cumulative = br['cumulative_coverage']\n",
    "        if cumulative >= target_jobs:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTo cover {target_coverage:.0%} of jobs ({target_jobs}/{total_jobs}), use these benchmarks in order:\")\n",
    "    for i, bench in enumerate(recommended, 1):\n",
    "        br = benchmark_ranking[i-1]\n",
    "        print(f\"{i}. {bench} (+{br['new_jobs_covered']} new jobs)\")\n",
    "    \n",
    "    final_coverage = cumulative / total_jobs if total_jobs else 0\n",
    "    print(f\"\\nFinal coverage: {cumulative}/{total_jobs} jobs ({final_coverage:.1%})\")\n",
    "    \n",
    "    # Save results\n",
    "    output = {\n",
    "        \"config\": {\n",
    "            \"similarity_threshold\": SIMILARITY_THRESHOLD\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"total_leaves\": len(all_leaves),\n",
    "            \"leaves_with_jobs\": len(leaves_with_jobs),\n",
    "            \"leaves_with_both\": len(leaves_with_both),\n",
    "            \"coverage\": coverage,\n",
    "            \"total_jobs\": total_jobs,\n",
    "            \"jobs_with_high_quality_tasks\": jobs_with_high_quality_tasks,\n",
    "            \"quality_coverage\": quality_coverage\n",
    "        },\n",
    "        \"distribution\": dict(task_count_dist),\n",
    "        \"benchmark_ranking\": benchmark_ranking,\n",
    "        \"recommended_benchmarks\": recommended,\n",
    "        \"job_details\": job_coverage_details\n",
    "    }\n",
    "    \n",
    "    save_json(output, OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762202c-cd55-40c3-ac89-bd9c575edf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
