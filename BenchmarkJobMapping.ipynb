{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b92892-a93e-4bc9-9135-f7c5ff6e2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating taxonomy with jobs from O*Net\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import openai\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"\"\n",
    "BASE_URL = \"\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "INPUT_FILE = \"job_tasks.csv\"\n",
    "OUTPUT_FILE = \"job_tasks_with_skills.csv\"\n",
    "TAXONOMY_FILE = \"taxonomy.json\"\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# -------------------------------\n",
    "# OpenAI Client\n",
    "# -------------------------------\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Load CSV\n",
    "# -------------------------------\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Add a persistent unique TaskID if not already there\n",
    "if \"TaskID\" not in df.columns:\n",
    "    df.insert(0, \"TaskID\", range(1, len(df) + 1))\n",
    "\n",
    "# Ensure required columns exist\n",
    "required = {\"Task\", \"Automation Desire Rating\", \"Job Security Rating\", \"Enjoyment Rating\", \"Occupation (O*NET-SOC Title)\"}\n",
    "if not required.issubset(df.columns):\n",
    "    raise ValueError(f\"CSV must contain columns: {required}\")\n",
    "\n",
    "# Add new column for skills if missing\n",
    "if \"generic_skill\" not in df.columns:\n",
    "    df[\"generic_skill\"] = None\n",
    "\n",
    "# -------------------------------\n",
    "# Load or init taxonomy\n",
    "# -------------------------------\n",
    "if os.path.exists(TAXONOMY_FILE):\n",
    "    with open(TAXONOMY_FILE, \"r\") as f:\n",
    "        taxonomy = json.load(f)\n",
    "else:\n",
    "    taxonomy = {\"root\": {}}\n",
    "\n",
    "# -------------------------------\n",
    "# Utility: atomic file write\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, filename):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False) as tmp:\n",
    "        json.dump(data, tmp, indent=2)\n",
    "        tmp_path = tmp.name\n",
    "    shutil.move(tmp_path, filename)\n",
    "\n",
    "def atomic_write_csv(df, filename):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".csv\") as tmp:\n",
    "        df.to_csv(tmp.name, index=False)\n",
    "        tmp_path = tmp.name\n",
    "    shutil.move(tmp_path, filename)\n",
    "\n",
    "# -------------------------------\n",
    "# Prompt function\n",
    "# -------------------------------\n",
    "def classify_tasks(batch, taxonomy):\n",
    "    \"\"\"Send a batch of tasks to the LLM and return skill mappings + updated taxonomy.\"\"\"\n",
    "\n",
    "    tasks_str = \"\\n\".join(\n",
    "        [f\"ID:{row['TaskID']} - {row['Task']} ({row['Occupation (O*NET-SOC Title)']})\"\n",
    "         for _, row in batch.iterrows()]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in occupational task analysis.\n",
    "\n",
    "Here is the current taxonomy (JSON):\n",
    "<taxonomy>\n",
    "{json.dumps(taxonomy, indent=2)}\n",
    "</taxonomy>\n",
    "\n",
    "Here are new job tasks with job titles and IDs:\n",
    "{tasks_str}\n",
    "\n",
    "Instructions:\n",
    "1. For each task, provide a generic skill description that captures the core action, while preserving necessary domain nuance.\n",
    "   - Example: \"Maintain records of drilling operations\" → \"Maintain technical records\"\n",
    "   - Example: \"Review legal case documents\" → \"Review long formal documents\"\n",
    "2. Place each skill under the most relevant existing category in the taxonomy.\n",
    "3. If no suitable category exists, add a new subcategory under the most relevant parent.\n",
    "4. Output two sections:\n",
    "   <skills>\n",
    "   - ID:xxx → Generic skill (taxonomy_path)\n",
    "   </skills>\n",
    "\n",
    "   <updated_taxonomy>\n",
    "   {{...json updated taxonomy...}}\n",
    "   </updated_taxonomy>\n",
    "\"\"\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert at building skill taxonomies.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            content = response.choices[0].message.content if response.choices else \"\"\n",
    "            if not content:\n",
    "                raise ValueError(\"Empty response\")\n",
    "            print(content)\n",
    "\n",
    "            # ----------------- Parse skills -----------------\n",
    "            skills_map = {}\n",
    "            if \"<skills>\" in content and \"</skills>\" in content:\n",
    "                skills_section = content.split(\"<skills>\")[1].split(\"</skills>\")[0].strip()\n",
    "                for line in skills_section.splitlines():\n",
    "                    line = line.strip()\n",
    "                    if line.startswith(\"- ID:\") or line.startswith(\"ID:\"):\n",
    "                        try:\n",
    "                            id_part, skill_part = line.replace(\"-\", \"\").split(\"→\", 1)\n",
    "                            task_id = int(id_part.strip().split(\":\")[1])\n",
    "                            skills_map[task_id] = skill_part.strip()\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            # ----------------- Parse taxonomy -----------------\n",
    "            updated_taxonomy = taxonomy\n",
    "            if \"<updated_taxonomy>\" in content and \"</updated_taxonomy>\" in content:\n",
    "                taxonomy_section = content.split(\"<updated_taxonomy>\")[1].split(\"</updated_taxonomy>\")[0].strip()\n",
    "                try:\n",
    "                    updated_taxonomy = json.loads(taxonomy_section)\n",
    "                except Exception as e:\n",
    "                    print(\"Warning: could not parse taxonomy JSON:\", e)\n",
    "                    updated_taxonomy = taxonomy  # keep last good taxonomy\n",
    "\n",
    "            return skills_map, updated_taxonomy\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2 ** attempt)  # exponential backoff\n",
    "\n",
    "    print(\"❌ Failed after max retries\")\n",
    "    return {}, taxonomy\n",
    "\n",
    "# -------------------------------\n",
    "# Process in batches\n",
    "# -------------------------------\n",
    "unprocessed = df[df[\"generic_skill\"].isna()].copy()\n",
    "\n",
    "for start in range(0, len(unprocessed), BATCH_SIZE):\n",
    "    batch = unprocessed.iloc[start:start+BATCH_SIZE]\n",
    "    print(f\"Processing {start}–{start+len(batch)-1}...\")\n",
    "\n",
    "    skills_map, taxonomy = classify_tasks(batch, taxonomy)\n",
    "\n",
    "    # Update DataFrame\n",
    "    for task_id, skill in skills_map.items():\n",
    "        df.loc[df[\"TaskID\"] == task_id, \"generic_skill\"] = skill\n",
    "\n",
    "    # Save progress atomically\n",
    "    atomic_write_csv(df, OUTPUT_FILE)\n",
    "    atomic_write_json(taxonomy, TAXONOMY_FILE)\n",
    "\n",
    "print(\"✅ Done! Results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2f46f-77ec-4f6f-90e4-6747bed2e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using benchmark tasks, map them to the common taxonomy\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import tempfile\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import openai\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"\"\n",
    "BASE_URL = \"\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "BENCHMARK_FILE = \"biocoder_tasks.txt\"\n",
    "TAXONOMY_FILE = \"../taxonomy_restructured.json\"\n",
    "TAXONOMY_BACKUP_FILE = \"../taxonomy_restructured.json.backup\"\n",
    "OUTPUT_FILE = \"biocoder_tasks_mapping_re.json\"\n",
    "CHECKPOINT_FILE = \"biocoder_tasks_progress_re.json\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "INITIAL_SAMPLE_MIN = 20\n",
    "ALPHA = 0.1\n",
    "DESIRED_COVERAGE = 0.90\n",
    "MAX_CONSEC_NO_NEW = 10\n",
    "SAMPLE_SEED = 42\n",
    "\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, path):\n",
    "    \"\"\"Safely write JSON with temp + replace.\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf:\n",
    "        json.dump(data, tf, indent=2)\n",
    "        tmp = tf.name\n",
    "    shutil.move(tmp, path)\n",
    "\n",
    "def chao1(S_obs, freq_values):\n",
    "    \"\"\"Chao1 estimator for total species richness.\"\"\"\n",
    "    if len(freq_values) < 2:\n",
    "        return float('inf')\n",
    "    \n",
    "    f1 = sum(1 for v in freq_values if v == 1)\n",
    "    f2 = sum(1 for v in freq_values if v == 2)\n",
    "    \n",
    "    if f1 == 0:\n",
    "        return S_obs\n",
    "    \n",
    "    if f2 == 0:\n",
    "        return S_obs + f1 * (f1 - 1) / 2 if f1 > 1 else S_obs + f1\n",
    "    \n",
    "    return S_obs + f1 * f1 / (2 * f2)\n",
    "\n",
    "# -------------------------------\n",
    "# Load benchmark tasks\n",
    "# -------------------------------\n",
    "with open(BENCHMARK_FILE, \"r\") as f:\n",
    "    lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "benchmark_tasks = []\n",
    "for line in lines:\n",
    "    if \":\" in line:\n",
    "        task_id, instruction = line.split(\":\", 1)\n",
    "        benchmark_tasks.append({\n",
    "            \"benchmark_task_id\": task_id.strip(),\n",
    "            \"instruction\": instruction.strip()\n",
    "        })\n",
    "\n",
    "# -------------------------------\n",
    "# Load taxonomy\n",
    "# -------------------------------\n",
    "if not os.path.exists(TAXONOMY_BACKUP_FILE) and os.path.exists(TAXONOMY_FILE):\n",
    "    print(f\"[BACKUP] Creating backup: {TAXONOMY_BACKUP_FILE}\")\n",
    "    shutil.copy2(TAXONOMY_FILE, TAXONOMY_BACKUP_FILE)\n",
    "\n",
    "with open(TAXONOMY_FILE, \"r\") as f:\n",
    "    taxonomy = json.load(f)\n",
    "\n",
    "def init_leaf_dict(node):\n",
    "    if isinstance(node, dict):\n",
    "        for k, v in list(node.items()):\n",
    "            if v == {}:\n",
    "                node[k] = {\"tasks\": []}\n",
    "            else:\n",
    "                init_leaf_dict(v)\n",
    "\n",
    "init_leaf_dict(taxonomy[\"root\"])\n",
    "\n",
    "# -------------------------------\n",
    "# State (resume if checkpoint exists)\n",
    "# -------------------------------\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    remaining_indices = state[\"remaining_indices\"]\n",
    "    discovered_leaves = Counter({tuple(k): v for k, v in state[\"discovered_leaves\"].items()})\n",
    "    consec_no_new = state[\"consec_no_new\"]\n",
    "    mapping_log = state[\"mapping_log\"]\n",
    "    taxonomy = state[\"taxonomy\"]\n",
    "    print(f\"[RESUME] Sampled: {len(mapping_log)}, Remaining: {len(remaining_indices)}, Unique leaves: {len(discovered_leaves)}\")\n",
    "else:\n",
    "    random.seed(SAMPLE_SEED)\n",
    "    remaining_indices = list(range(len(benchmark_tasks)))\n",
    "    random.shuffle(remaining_indices)\n",
    "    discovered_leaves = Counter()\n",
    "    consec_no_new = 0\n",
    "    mapping_log = {}\n",
    "    print(f\"[START] Total tasks: {len(benchmark_tasks)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# LLM mapping function\n",
    "# -------------------------------\n",
    "def map_tasks_to_taxonomy(tasks_batch, taxonomy):\n",
    "    tasks_str = \"\\n\".join(\n",
    "        [f\"{i+1}. [{t['benchmark_task_id']}] {t['instruction']}\" for i, t in enumerate(tasks_batch)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert occupational skill mapper.\n",
    "\n",
    "Here is the current taxonomy (JSON):\n",
    "<taxonomy>\n",
    "{json.dumps(taxonomy, indent=2)}\n",
    "</taxonomy>\n",
    "\n",
    "You are given the following benchmark tasks:\n",
    "{tasks_str}\n",
    "\n",
    "Instructions:\n",
    "- For each benchmark task, assign one or more relevant leaves (skills) from the taxonomy. The skills must be directly the function of the task as described by the task description.\n",
    "- Do NOT extrapolate the task description to related skills, but map exactly the skills required to the task.\n",
    "- If there is no direct skill that is related to the task, or if the task is not sufficiently representative of the skill, output \"N/A\" for the task.\n",
    "- Multiple leaves can be assigned per task.\n",
    "- Only use exact existing categories, do NOT invent new ones.\n",
    "- Output must include:\n",
    "<task_mappings>\n",
    "- benchmark_task_id: [ \"root/Category/Subcategory\", ... ]\n",
    "</task_mappings>\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in skill mapping and taxonomy assignment.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content if response.choices else \"\"\n",
    "    print(content)\n",
    "\n",
    "    task_map = {}\n",
    "    if \"<task_mappings>\" in content and \"</task_mappings>\" in content:\n",
    "        mapping_section = content.split(\"<task_mappings>\")[1].split(\"</task_mappings>\")[0].strip()\n",
    "        \n",
    "        # Try to parse the entire mapping section as one JSON-like structure\n",
    "        try:\n",
    "            lines = mapping_section.splitlines()\n",
    "            json_str = \"{\"\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"-\") and \":\" in line:\n",
    "                    parts = line[1:].split(\":\", 1)\n",
    "                    task_id = parts[0].strip()\n",
    "                    value_part = parts[1].strip()\n",
    "                    if json_str != \"{\":\n",
    "                        json_str += \",\"\n",
    "                    json_str += f'\"{task_id}\": {value_part}'\n",
    "                elif line and not line.startswith(\"-\"):\n",
    "                    json_str += line\n",
    "            json_str += \"}\"\n",
    "            \n",
    "            parsed = json.loads(json_str.replace(\"'\", '\"'))\n",
    "            task_map.update(parsed)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed bulk parsing, falling back to line-by-line: {e}\")\n",
    "            \n",
    "            current_task_id = None\n",
    "            current_value = \"\"\n",
    "            \n",
    "            for line in mapping_section.splitlines():\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"-\") and \":\" in line:\n",
    "                    if current_task_id:\n",
    "                        try:\n",
    "                            if current_value.upper() == \"N/A\":\n",
    "                                task_map[current_task_id] = []\n",
    "                            else:\n",
    "                                paths = json.loads(current_value.replace(\"'\", '\"'))\n",
    "                                task_map[current_task_id] = paths\n",
    "                        except Exception as ex:\n",
    "                            print(f\"⚠️ Failed parsing task {current_task_id}: {ex}\")\n",
    "                    \n",
    "                    parts = line[1:].split(\":\", 1)\n",
    "                    current_task_id = parts[0].strip()\n",
    "                    current_value = parts[1].strip()\n",
    "                elif line and current_task_id:\n",
    "                    current_value += \" \" + line\n",
    "            \n",
    "            if current_task_id:\n",
    "                try:\n",
    "                    if current_value.upper() == \"N/A\":\n",
    "                        task_map[current_task_id] = []\n",
    "                    else:\n",
    "                        paths = json.loads(current_value.replace(\"'\", '\"'))\n",
    "                        task_map[current_task_id] = paths\n",
    "                except Exception as ex:\n",
    "                    print(f\"⚠️ Failed parsing task {current_task_id}: {ex}\")\n",
    "\n",
    "    return task_map\n",
    "\n",
    "# -------------------------------\n",
    "# Helper to check path exists\n",
    "# -------------------------------\n",
    "def path_exists(taxonomy, path):\n",
    "    try:\n",
    "        node = taxonomy[\"root\"]\n",
    "        for p in path:\n",
    "            node = node[p]\n",
    "        return \"tasks\" in node\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "# -------------------------------\n",
    "# Normalize and clean path\n",
    "# -------------------------------\n",
    "def normalize_path(path_str):\n",
    "    \"\"\"\n",
    "    Normalize path: remove 'root/', strip '/tasks' suffix, handle missing root.\n",
    "    Returns tuple of path components.\n",
    "    \"\"\"\n",
    "    path_str = path_str.strip().strip('/')\n",
    "    \n",
    "    # Remove \"root/\" prefix if present\n",
    "    if path_str.startswith(\"root/\"):\n",
    "        path_str = path_str[5:]\n",
    "    \n",
    "    # Remove \"/tasks\" suffix if present\n",
    "    if path_str.endswith(\"/tasks\"):\n",
    "        path_str = path_str[:-6]\n",
    "    \n",
    "    # Skip empty paths\n",
    "    if not path_str:\n",
    "        return None\n",
    "        \n",
    "    return tuple(path_str.split(\"/\"))\n",
    "\n",
    "# -------------------------------\n",
    "# Sampling loop\n",
    "# -------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH SAMPLING WITH COVERAGE-BASED STOPPING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "while remaining_indices:\n",
    "    # Get batch\n",
    "    batch_indices = [remaining_indices.pop(0) for _ in range(min(BATCH_SIZE, len(remaining_indices)))]\n",
    "    batch_tasks = [benchmark_tasks[i] for i in batch_indices]\n",
    "\n",
    "    print(f\"\\n[BATCH] Processing {len(batch_tasks)} tasks...\")\n",
    "    task_map = map_tasks_to_taxonomy(batch_tasks, taxonomy)\n",
    "\n",
    "    new_leaf_discovered = False\n",
    "    batch_new_leaves = []\n",
    "    \n",
    "    for task_id, paths in task_map.items():\n",
    "        mapping_log[task_id] = paths\n",
    "        \n",
    "        for path_str in paths:\n",
    "            path = normalize_path(path_str)\n",
    "            \n",
    "            if path is None:\n",
    "                print(f\"⚠️ Skipping empty path for task {task_id}\")\n",
    "                continue\n",
    "            \n",
    "            if path_exists(taxonomy, path):\n",
    "                # Navigate to node and add task\n",
    "                node = taxonomy[\"root\"]\n",
    "                for p in path:\n",
    "                    node = node[p]\n",
    "                \n",
    "                if task_id not in node[\"tasks\"]:\n",
    "                    node[\"tasks\"].append(task_id)\n",
    "                \n",
    "                # Track leaf discovery\n",
    "                if path not in discovered_leaves:\n",
    "                    discovered_leaves[path] = 1\n",
    "                    new_leaf_discovered = True\n",
    "                    batch_new_leaves.append(path)\n",
    "                else:\n",
    "                    discovered_leaves[path] += 1\n",
    "            else:\n",
    "                print(f\"⚠️ Invalid path suggested by LLM: {path_str}\")\n",
    "\n",
    "    # Update consecutive no-new counter\n",
    "    if new_leaf_discovered:\n",
    "        consec_no_new = 0\n",
    "        print(f\"✅ Discovered {len(batch_new_leaves)} new leaf/leaves:\")\n",
    "        for leaf in batch_new_leaves:\n",
    "            print(f\"  - {'/'.join(leaf)}\")\n",
    "    else:\n",
    "        consec_no_new += 1\n",
    "        print(f\"✅ No new leaves discovered ({consec_no_new} consecutive batches)\")\n",
    "\n",
    "    # Calculate statistics\n",
    "    n_sampled = len(mapping_log)\n",
    "    S_obs = len(discovered_leaves)\n",
    "    freq_values = list(discovered_leaves.values())\n",
    "    \n",
    "    p_new = (sum(1 for v in freq_values if v == 1) / n_sampled) if n_sampled > 0 else 1\n",
    "    S_chao = chao1(S_obs, freq_values) if n_sampled > 0 else float(\"inf\")\n",
    "    coverage = S_obs / S_chao if S_chao and math.isfinite(S_chao) else 0\n",
    "\n",
    "    print(f\"\\n[STATS] Sampled: {n_sampled}, Unique leaves: {S_obs}, Chao1: {S_chao:.1f}, Coverage: {coverage:.2%}, P(new): {p_new:.3f}\")\n",
    "\n",
    "    # Stopping conditions\n",
    "    stop_reason = None\n",
    "    if n_sampled >= INITIAL_SAMPLE_MIN:\n",
    "        if p_new <= ALPHA:\n",
    "            stop_reason = f\"Low new discovery probability: {p_new:.3f} <= {ALPHA}\"\n",
    "        elif coverage >= DESIRED_COVERAGE and math.isfinite(S_chao):\n",
    "            stop_reason = f\"Desired coverage reached: {coverage:.2%} >= {DESIRED_COVERAGE:.2%}\"\n",
    "        elif consec_no_new >= MAX_CONSEC_NO_NEW:\n",
    "            stop_reason = f\"No new leaves for {consec_no_new} consecutive batches\"\n",
    "    else:\n",
    "        print(f\"[INFO] Continuing to reach minimum sample size ({n_sampled}/{INITIAL_SAMPLE_MIN})\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    atomic_write_json({\n",
    "        \"remaining_indices\": remaining_indices,\n",
    "        \"discovered_leaves\": {str(list(k)): v for k, v in discovered_leaves.items()},\n",
    "        \"consec_no_new\": consec_no_new,\n",
    "        \"mapping_log\": mapping_log,\n",
    "        \"taxonomy\": taxonomy\n",
    "    }, CHECKPOINT_FILE)\n",
    "    \n",
    "    if stop_reason:\n",
    "        print(f\"\\n[STOPPING] {stop_reason}\")\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# Final save\n",
    "# -------------------------------\n",
    "atomic_write_json({\n",
    "    \"taxonomy\": taxonomy,\n",
    "    \"mapping_log\": mapping_log,\n",
    "    \"statistics\": {\n",
    "        \"total_sampled\": len(mapping_log),\n",
    "        \"unique_leaves\": len(discovered_leaves),\n",
    "        \"chao1_estimate\": S_chao if math.isfinite(S_chao) else None,\n",
    "        \"coverage\": coverage\n",
    "    }\n",
    "}, OUTPUT_FILE)\n",
    "\n",
    "atomic_write_json(taxonomy, TAXONOMY_FILE)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY:\")\n",
    "print(f\"  Tasks sampled: {len(mapping_log)}/{len(benchmark_tasks)}\")\n",
    "print(f\"  Unique leaves discovered: {len(discovered_leaves)}\")\n",
    "print(f\"  Chao1 estimate: {S_chao:.1f}\")\n",
    "print(f\"  Coverage: {coverage:.2%}\")\n",
    "print(f\"  Mapping saved to: {OUTPUT_FILE}\")\n",
    "print(f\"  Updated taxonomy: {TAXONOMY_FILE}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_FILE}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3de015-8205-4fe0-a474-dccbabbdcfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging jobs and tasks into one taxonomy\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "JOB_SKILLS_FILE = \"../job_tasks_with_skills.csv\"\n",
    "TAXONOMY_FILE = \"taxonomy_with_instructions.json\"\n",
    "TAXONOMY_INFO_FILE = \"../taxonomy_restructure_info.json\"\n",
    "OUTPUT_FILE = \"taxonomy_with_jobs.json\"\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "def get_node(taxonomy, path_str):\n",
    "    \"\"\"Navigate to a node in taxonomy given a path string.\"\"\"\n",
    "    parts = path_str.split(\"/\")\n",
    "    if parts[0] != \"root\":\n",
    "        return None\n",
    "    \n",
    "    node = taxonomy[\"root\"]\n",
    "    for part in parts[1:]:\n",
    "        if part in node:\n",
    "            node = node[part]\n",
    "        else:\n",
    "            return None\n",
    "    return node\n",
    "\n",
    "# -------------------------------\n",
    "# Load data\n",
    "# -------------------------------\n",
    "print(\"[LOAD] Loading taxonomy and job data...\")\n",
    "\n",
    "taxonomy = load_json(TAXONOMY_FILE)\n",
    "taxonomy_info = load_json(TAXONOMY_INFO_FILE)\n",
    "task_mapping = taxonomy_info.get(\"task_mapping\", {})\n",
    "# Collect jobs by path\n",
    "jobs_by_path = defaultdict(list)\n",
    "\n",
    "print(\"[LOAD] Reading job skills CSV...\")\n",
    "with open(JOB_SKILLS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        task_id = row[\"TaskID\"]\n",
    "        task_text = row[\"Task\"]\n",
    "        occupation = row[\"Occupation (O*NET-SOC Title)\"]\n",
    "        automation = row[\"Automation Desire Rating\"]\n",
    "        job_security = row[\"Job Security Rating\"]\n",
    "        enjoyment = row[\"Enjoyment Rating\"]\n",
    "        generic_skill = row[\"generic_skill\"]\n",
    "        \n",
    "        # Extract path from generic_skill (format: \"Description (Path)\")\n",
    "        if \"(\" in generic_skill and \")\" in generic_skill:\n",
    "            path_part = generic_skill.split(\"(\")[-1].split(\")\")[0]\n",
    "            \n",
    "            # Convert to taxonomy path format\n",
    "            path_components = [p.strip() for p in path_part.split(\">\")]\n",
    "            original_path = \"root/\" + \"/\".join(path_components)\n",
    "            mapped_path = original_path\n",
    "            # Fix duplicate root/ prefix if present\n",
    "            if mapped_path.startswith(\"root/root/\"):\n",
    "                final_path = mapped_path[5:]  # Remove the first \"root/\"\n",
    "            else:\n",
    "                final_path = mapped_path\n",
    "            final_path = task_mapping.get(final_path.strip(), final_path)\n",
    "            \n",
    "            # Add job to the final path\n",
    "            jobs_by_path[final_path].append({\n",
    "                \"job_task_id\": task_id,\n",
    "                \"task\": task_text,\n",
    "                \"occupation\": occupation,\n",
    "                \"automation_desire\": automation,\n",
    "                \"job_security\": job_security,\n",
    "                \"enjoyment\": enjoyment\n",
    "            })\n",
    "\n",
    "print(f\"[LOAD] Found {len(jobs_by_path)} unique paths with jobs\")\n",
    "print(f\"[LOAD] Total job tasks: {sum(len(jobs) for jobs in jobs_by_path.values())}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Add jobs to taxonomy\n",
    "# -------------------------------\n",
    "print(\"\\n[PROCESS] Adding jobs to taxonomy nodes...\")\n",
    "\n",
    "jobs_added = 0\n",
    "paths_with_jobs = 0\n",
    "paths_not_found = []\n",
    "\n",
    "for path, jobs in jobs_by_path.items():\n",
    "    node = get_node(taxonomy, path)\n",
    "    if node is not None:\n",
    "        if \"jobs\" not in node:\n",
    "            node[\"jobs\"] = []\n",
    "        \n",
    "        # Add each job if not already present\n",
    "        for job in jobs:\n",
    "            # Check if job already exists (by job_task_id)\n",
    "            existing_ids = [j[\"job_task_id\"] for j in node[\"jobs\"] if isinstance(j, dict)]\n",
    "            if job[\"job_task_id\"] not in existing_ids:\n",
    "                node[\"jobs\"].append(job)\n",
    "                jobs_added += 1\n",
    "        \n",
    "        paths_with_jobs += 1\n",
    "    else:\n",
    "        paths_not_found.append(path)\n",
    "        print(f\"⚠️ Path not found in taxonomy: {path}\")\n",
    "\n",
    "print(f\"\\n[SUMMARY]\")\n",
    "print(f\"  Paths with jobs added: {paths_with_jobs}\")\n",
    "print(f\"  Total jobs added: {jobs_added}\")\n",
    "print(f\"  Paths not found: {len(paths_not_found)}\")\n",
    "\n",
    "if paths_not_found:\n",
    "    print(f\"\\n[WARNING] The following paths were not found:\")\n",
    "    for path in paths_not_found[:10]:  # Show first 10\n",
    "        print(f\"  - {path}\")\n",
    "    if len(paths_not_found) > 10:\n",
    "        print(f\"  ... and {len(paths_not_found) - 10} more\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save updated taxonomy\n",
    "# -------------------------------\n",
    "save_json(taxonomy, OUTPUT_FILE)\n",
    "print(f\"\\n✅ Updated taxonomy saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Generate statistics\n",
    "# -------------------------------\n",
    "def count_jobs_in_taxonomy(node):\n",
    "    \"\"\"Recursively count jobs in taxonomy.\"\"\"\n",
    "    count = 0\n",
    "    if isinstance(node, dict):\n",
    "        if \"jobs\" in node:\n",
    "            count += len([j for j in node[\"jobs\"] if isinstance(j, dict)])\n",
    "        for k, v in node.items():\n",
    "            if k not in [\"tasks\", \"jobs\"]:\n",
    "                count += count_jobs_in_taxonomy(v)\n",
    "    return count\n",
    "\n",
    "total_jobs = count_jobs_in_taxonomy(taxonomy[\"root\"])\n",
    "print(f\"\\n[STATS] Total jobs in taxonomy: {total_jobs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c0f5d-e245-4f13-8ef5-cb0731fdc8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check similarity between tasks and jobs on same taxonomy leaves\n",
    "\"\"\"\n",
    "Integrate new benchmark tasks into existing taxonomy with job similarities.\n",
    "This will:\n",
    "1. Add new benchmark tasks to taxonomy_with_similarity.json\n",
    "2. Recalculate similarities only for affected leaves\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "import openai\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "API_KEY = \"\"\n",
    "BASE_URL = \"\"\n",
    "MODEL_NAME = \"gpt-5\"\n",
    "\n",
    "TAXONOMY_FILE = \"taxonomy_with_similarity.json\"  # Start from existing file with similarities\n",
    "OUTPUT_FILE = \"taxonomy_with_similarity_updated.json\"\n",
    "CHECKPOINT_FILE = \"integration_progress.json\"\n",
    "\n",
    "TASK_BATCH_SIZE = 10\n",
    "\n",
    "client = openai.OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def atomic_write_json(data, path):\n",
    "    with tempfile.NamedTemporaryFile(\"w\", delete=False, suffix=\".json\") as tf:\n",
    "        json.dump(data, tf, indent=2)\n",
    "        tmp = tf.name\n",
    "    shutil.move(tmp, path)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_node(taxonomy, path_list):\n",
    "    node = taxonomy\n",
    "    for p in path_list:\n",
    "        if p not in node:\n",
    "            return None\n",
    "        node = node[p]\n",
    "    return node\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Merge new benchmark tasks\n",
    "# -------------------------------\n",
    "def merge_new_benchmarks(taxonomy):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: MERGING NEW BENCHMARK TASKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    affected_leaves = set()\n",
    "    \n",
    "    task_files = sorted(glob(\"*_tasks.txt\"))\n",
    "    print(f\"\\nFound {len(task_files)} benchmark file(s)\")\n",
    "    \n",
    "    for task_file in task_files:\n",
    "        base = task_file.replace(\"_tasks.txt\", \"\")\n",
    "        mapping_file = f\"{base}_tasks_mapping_re.json\"\n",
    "        \n",
    "        if not os.path.exists(mapping_file):\n",
    "            print(f\"Skipping {base}: no mapping file\")\n",
    "            continue\n",
    "        \n",
    "        benchmark = base\n",
    "        print(f\"\\nProcessing: {benchmark}\")\n",
    "        \n",
    "        # Load tasks\n",
    "        tasks = {}\n",
    "        with open(task_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \":\" in line:\n",
    "                    tid, inst = line.split(\":\", 1)\n",
    "                    tasks[tid.strip()] = inst.strip()\n",
    "        \n",
    "        print(f\"  Loaded {len(tasks)} tasks\")\n",
    "        \n",
    "        # Load mapping\n",
    "        mapping = load_json(mapping_file)\n",
    "        mapping_tax = mapping.get(\"taxonomy\", {}).get(\"root\", {})\n",
    "        \n",
    "        # Process mapping\n",
    "        added = 0\n",
    "        \n",
    "        def traverse(node, path):\n",
    "            nonlocal added\n",
    "            if not isinstance(node, dict):\n",
    "                return\n",
    "            \n",
    "            if \"tasks\" in node:\n",
    "                task_ids = [t for t in node[\"tasks\"] if isinstance(t, str) and t in tasks]\n",
    "                if task_ids:\n",
    "                    master = get_node(taxonomy, path)\n",
    "                    if master:\n",
    "                        # Initialize\n",
    "                        if \"tasks\" not in master:\n",
    "                            master[\"tasks\"] = []\n",
    "                        \n",
    "                        # Clean string IDs\n",
    "                        master[\"tasks\"] = [t for t in master[\"tasks\"] if isinstance(t, dict)]\n",
    "                        \n",
    "                        # Get existing IDs\n",
    "                        existing = {t.get(\"task_id\") for t in master[\"tasks\"]}\n",
    "                        \n",
    "                        # Add new tasks\n",
    "                        for tid in task_ids:\n",
    "                            if tid not in existing:\n",
    "                                master[\"tasks\"].append({\n",
    "                                    \"benchmark\": benchmark,\n",
    "                                    \"task_id\": tid,\n",
    "                                    \"instruction\": tasks[tid],\n",
    "                                    \"job_similarities\": []  # Will be filled later\n",
    "                                })\n",
    "                                added += 1\n",
    "                                affected_leaves.add(tuple(path))\n",
    "            \n",
    "            for k, v in node.items():\n",
    "                if k != \"tasks\":\n",
    "                    traverse(v, path + [k])\n",
    "        \n",
    "        traverse(mapping_tax, [\"root\"])\n",
    "        print(f\"  Added {added} new task objects\")\n",
    "    \n",
    "    print(f\"\\n✓ Total affected leaves: {len(affected_leaves)}\")\n",
    "    return affected_leaves\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Recalculate similarities for affected leaves\n",
    "# -------------------------------\n",
    "def calculate_similarity_batch(benchmark_tasks, job_tasks, skill_path):\n",
    "    tasks_str = \"\\n\".join([\n",
    "        f\"{i+1}. [ID: {t['task_id']}] {t['instruction']}\"\n",
    "        for i, t in enumerate(benchmark_tasks)\n",
    "    ])\n",
    "    \n",
    "    jobs_str = \"\\n\".join([\n",
    "        f\"{i+1}. [ID: {j['job_task_id']}] {j['task']} (Occupation: {j['occupation']})\"\n",
    "        for i, j in enumerate(job_tasks)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert at comparing and rating task similarity in occupational contexts.\n",
    "\n",
    "You are analyzing tasks within the skill category: {skill_path}\n",
    "\n",
    "Here are BENCHMARK TASKS (from automated task benchmarks):\n",
    "{tasks_str}\n",
    "\n",
    "Here are JOB TASKS (from real occupations):\n",
    "{jobs_str}\n",
    "\n",
    "For each BENCHMARK TASK, evaluate its similarity to each JOB TASK on a scale of 0-10:\n",
    "- 0: Completely unrelated\n",
    "- 1-3: Minimal relation (shares very broad concepts only)\n",
    "- 4-6: Moderate relation (overlapping skills but different contexts/goals)\n",
    "- 7-9: Strong relation (similar skills and contexts, different specifics)\n",
    "- 10: Nearly identical (same skills, same context, same goal)\n",
    "\n",
    "Consider:\n",
    "- What specific skills are required?\n",
    "- How well is the benchmark task representative of the skills required for the job?\n",
    "- How close in terms of complexity is the benchmark task to the job task?\n",
    "- What is the context/domain?\n",
    "- What is the end goal/outcome?\n",
    "- How transferable are the skills?\n",
    "\n",
    "Output format:\n",
    "<similarities>\n",
    "benchmark_task_id: [\n",
    "  {{\"job_task_id\": \"X\", \"score\": Y, \"reasoning\": \"brief explanation\"}},\n",
    "  ...\n",
    "]\n",
    "</similarities>\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in occupational task analysis and skill matching.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content if response.choices else \"\"\n",
    "    \n",
    "    # Parse response\n",
    "    similarity_map = {}\n",
    "    if \"<similarities>\" in content and \"</similarities>\" in content:\n",
    "        sim_section = content.split(\"<similarities>\")[1].split(\"</similarities>\")[0].strip()\n",
    "        \n",
    "        current_task_id = None\n",
    "        current_json = \"\"\n",
    "        \n",
    "        for line in sim_section.splitlines():\n",
    "            line = line.strip()\n",
    "            \n",
    "            if \":\" in line and line.endswith(\"[\"):\n",
    "                if current_task_id and current_json:\n",
    "                    try:\n",
    "                        similarity_map[current_task_id] = json.loads(current_json)\n",
    "                    except:\n",
    "                        pass\n",
    "                current_task_id = line.split(\":\")[0].strip()\n",
    "                current_json = \"[\"\n",
    "            elif line.startswith(\"]\"):\n",
    "                current_json += \"]\"\n",
    "                if current_task_id:\n",
    "                    try:\n",
    "                        similarity_map[current_task_id] = json.loads(current_json)\n",
    "                    except:\n",
    "                        pass\n",
    "                current_task_id = None\n",
    "                current_json = \"\"\n",
    "            elif current_task_id:\n",
    "                current_json += line\n",
    "        \n",
    "        if current_task_id and current_json:\n",
    "            try:\n",
    "                if not current_json.endswith(\"]\"):\n",
    "                    current_json += \"]\"\n",
    "                similarity_map[current_task_id] = json.loads(current_json)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return similarity_map\n",
    "\n",
    "def recalculate_similarities(taxonomy, affected_leaves):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: RECALCULATING SIMILARITIES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load checkpoint if exists\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        checkpoint = load_json(CHECKPOINT_FILE)\n",
    "        processed = set(checkpoint.get(\"processed_leaves\", []))\n",
    "    else:\n",
    "        processed = set()\n",
    "    \n",
    "    leaves_to_process = [\n",
    "        leaf for leaf in affected_leaves \n",
    "        if \"/\".join(leaf) not in processed\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nTotal affected leaves: {len(affected_leaves)}\")\n",
    "    print(f\"Already processed: {len(processed)}\")\n",
    "    print(f\"Remaining: {len(leaves_to_process)}\")\n",
    "    \n",
    "    total_comparisons = 0\n",
    "    \n",
    "    for idx, leaf_path in enumerate(leaves_to_process):\n",
    "        leaf_str = \"/\".join(leaf_path)\n",
    "        print(f\"\\n[{idx+1}/{len(leaves_to_process)}] {leaf_str}\")\n",
    "        \n",
    "        node = get_node(taxonomy, list(leaf_path))\n",
    "        if not node:\n",
    "            continue\n",
    "        \n",
    "        tasks = [t for t in node.get(\"tasks\", []) if isinstance(t, dict)]\n",
    "        jobs = [j for j in node.get(\"jobs\", []) if isinstance(j, dict)]\n",
    "        \n",
    "        # Find tasks without similarities\n",
    "        tasks_needing_sim = [t for t in tasks if not t.get(\"job_similarities\")]\n",
    "        \n",
    "        print(f\"  Tasks: {len(tasks)}, Jobs: {len(jobs)}, Need similarity: {len(tasks_needing_sim)}\")\n",
    "        \n",
    "        if not tasks_needing_sim or not jobs:\n",
    "            processed.add(leaf_str)\n",
    "            continue\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in range(0, len(tasks_needing_sim), TASK_BATCH_SIZE):\n",
    "            batch_end = min(batch_start + TASK_BATCH_SIZE, len(tasks_needing_sim))\n",
    "            task_batch = tasks_needing_sim[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"  Processing tasks {batch_start+1}-{batch_end}...\")\n",
    "            \n",
    "            try:\n",
    "                similarities = calculate_similarity_batch(task_batch, jobs, leaf_str)\n",
    "                \n",
    "                for task in task_batch:\n",
    "                    task_id = task[\"task_id\"]\n",
    "                    if task_id in similarities:\n",
    "                        task[\"job_similarities\"] = similarities[task_id]\n",
    "                    else:\n",
    "                        task[\"job_similarities\"] = []\n",
    "                \n",
    "                total_comparisons += len(task_batch) * len(jobs)\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Error: {e}\")\n",
    "        \n",
    "        processed.add(leaf_str)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        atomic_write_json({\n",
    "            \"processed_leaves\": list(processed),\n",
    "            \"taxonomy\": taxonomy\n",
    "        }, CHECKPOINT_FILE)\n",
    "    \n",
    "    print(f\"\\n✓ Completed {total_comparisons} comparisons\")\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"INTEGRATING NEW BENCHMARK TASKS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load existing taxonomy with similarities\n",
    "    print(f\"\\nLoading: {TAXONOMY_FILE}\")\n",
    "    taxonomy = load_json(TAXONOMY_FILE)\n",
    "    \n",
    "    # Step 1: Merge new tasks\n",
    "    affected_leaves = merge_new_benchmarks(taxonomy)\n",
    "    \n",
    "    # Step 2: Recalculate similarities only for affected leaves\n",
    "    if affected_leaves:\n",
    "        recalculate_similarities(taxonomy, affected_leaves)\n",
    "    else:\n",
    "        print(\"\\nNo new tasks added, nothing to recalculate\")\n",
    "    \n",
    "    # Save final result\n",
    "    atomic_write_json(taxonomy, OUTPUT_FILE)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPLETE\")\n",
    "    print(f\"Output: {OUTPUT_FILE}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1766bdb2-e28a-4cc3-8ef9-79d55e35fcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "JOB COVERAGE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Similarity threshold: 7\n",
      "Loading: taxonomy_with_similarity_updated.json\n",
      "\n",
      "Total leaves: 322\n",
      "Leaves with jobs: 275\n",
      "\n",
      "================================================================================\n",
      "METRIC 1: COVERAGE\n",
      "================================================================================\n",
      "Leaves with jobs that also have tasks: 187/275\n",
      "Coverage: 68.00%\n",
      "\n",
      "================================================================================\n",
      "METRIC 2: COVERAGE QUALITY\n",
      "================================================================================\n",
      "Total jobs analyzed: 673\n",
      "Jobs with high-quality tasks (>=7): 444\n",
      "Quality coverage: 65.97%\n",
      "\n",
      "================================================================================\n",
      "DISTRIBUTION: Tasks per Job\n",
      "================================================================================\n",
      "0 tasks: 229 jobs (34.0%)\n",
      "1 tasks: 128 jobs (19.0%)\n",
      "2 tasks: 80 jobs (11.9%)\n",
      "3 tasks: 46 jobs (6.8%)\n",
      "4 tasks: 33 jobs (4.9%)\n",
      "5 tasks: 24 jobs (3.6%)\n",
      "6 tasks: 11 jobs (1.6%)\n",
      "7 tasks: 17 jobs (2.5%)\n",
      "8 tasks: 15 jobs (2.2%)\n",
      "9 tasks: 8 jobs (1.2%)\n",
      "10 tasks: 4 jobs (0.6%)\n",
      "11 tasks: 4 jobs (0.6%)\n",
      "12 tasks: 5 jobs (0.7%)\n",
      "13 tasks: 7 jobs (1.0%)\n",
      "14 tasks: 4 jobs (0.6%)\n",
      "15 tasks: 4 jobs (0.6%)\n",
      "16 tasks: 1 jobs (0.1%)\n",
      "17 tasks: 2 jobs (0.3%)\n",
      "18 tasks: 2 jobs (0.3%)\n",
      "20 tasks: 7 jobs (1.0%)\n",
      "21 tasks: 2 jobs (0.3%)\n",
      "22 tasks: 1 jobs (0.1%)\n",
      "23 tasks: 2 jobs (0.3%)\n",
      "24 tasks: 1 jobs (0.1%)\n",
      "25 tasks: 1 jobs (0.1%)\n",
      "26 tasks: 1 jobs (0.1%)\n",
      "27 tasks: 1 jobs (0.1%)\n",
      "28 tasks: 2 jobs (0.3%)\n",
      "29 tasks: 1 jobs (0.1%)\n",
      "30 tasks: 3 jobs (0.4%)\n",
      "32 tasks: 2 jobs (0.3%)\n",
      "33 tasks: 1 jobs (0.1%)\n",
      "34 tasks: 2 jobs (0.3%)\n",
      "36 tasks: 1 jobs (0.1%)\n",
      "38 tasks: 2 jobs (0.3%)\n",
      "41 tasks: 1 jobs (0.1%)\n",
      "43 tasks: 1 jobs (0.1%)\n",
      "45 tasks: 1 jobs (0.1%)\n",
      "48 tasks: 1 jobs (0.1%)\n",
      "50 tasks: 1 jobs (0.1%)\n",
      "51 tasks: 1 jobs (0.1%)\n",
      "53 tasks: 1 jobs (0.1%)\n",
      "54 tasks: 1 jobs (0.1%)\n",
      "57 tasks: 1 jobs (0.1%)\n",
      "58 tasks: 1 jobs (0.1%)\n",
      "63 tasks: 1 jobs (0.1%)\n",
      "74 tasks: 1 jobs (0.1%)\n",
      "93 tasks: 1 jobs (0.1%)\n",
      "102 tasks: 1 jobs (0.1%)\n",
      "106 tasks: 1 jobs (0.1%)\n",
      "154 tasks: 1 jobs (0.1%)\n",
      "219 tasks: 1 jobs (0.1%)\n",
      "301 tasks: 1 jobs (0.1%)\n",
      "321 tasks: 1 jobs (0.1%)\n",
      "\n",
      "================================================================================\n",
      "JOBS WITH NO HIGH-QUALITY TASKS: 229\n",
      "================================================================================\n",
      "\n",
      "[253] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Order Fulfillment\n",
      "  Task: Contact suppliers to verify shipment details....\n",
      "\n",
      "[770] Securities, Commodities, and Financial Services Sales Agents\n",
      "  Path: root/Operations & Scheduling/Operations Core/Order Fulfillment\n",
      "  Task: Complete sales order tickets and submit for processing of client-requested transactions....\n",
      "\n",
      "[68] Medical Secretaries and Administrative Assistants\n",
      "  Path: root/Operations & Scheduling/Operations Core/Inventory Management\n",
      "  Task: Perform various clerical or administrative functions, such as ordering and maintaining an inventory ...\n",
      "\n",
      "[487] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Inventory Management\n",
      "  Task: Requisition and maintain inventories of materials or supplies necessary to meet production demands....\n",
      "\n",
      "[51] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Monitor meters, flow gauges, or other real-time data to ensure proper operation of biofuels producti...\n",
      "\n",
      "[132] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Adjust temperature, pressure, vacuum, level, flow rate, or transfer of biofuels to maintain processe...\n",
      "\n",
      "[158] Petroleum Engineers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Monitor production rates, and plan rework processes to improve production....\n",
      "\n",
      "[294] Production, Planning, and Expediting Clerks\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Compile information, such as production rates and progress, materials inventories, materials used, o...\n",
      "\n",
      "[634] Biofuels Production Managers\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Review logs, datasheets, or reports to ensure adequate production levels or to identify abnormalitie...\n",
      "\n",
      "[739] Editors\n",
      "  Path: root/Operations & Scheduling/Operations Core/Production Monitoring\n",
      "  Task: Oversee publication production, including artwork, layout, computer typesetting, and printing, ensur...\n",
      "\n",
      "... and 219 more\n",
      "\n",
      "================================================================================\n",
      "JOBS WITH BEST COVERAGE (Top 10)\n",
      "================================================================================\n",
      "\n",
      "[655] Secretaries and Administrative Assistants, Except Legal, Medical, and Executive (321 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Conduct searches to find needed information, using such sources as the Internet....\n",
      "\n",
      "[725] Librarians and Media Collections Specialists (301 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Search standard reference materials, including online sources and the Internet, to answer patrons' r...\n",
      "\n",
      "[247] Social Science Research Assistants (219 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Conduct internet-based and library research....\n",
      "\n",
      "[826] Librarians and Media Collections Specialists (154 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Locate unusual or unique information in response to specific requests....\n",
      "\n",
      "[28] News Analysts, Reporters, and Journalists (106 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Check reference materials, such as books, news files, or public records, to obtain relevant facts....\n",
      "\n",
      "[373] Medical Secretaries and Administrative Assistants (102 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Document Production & Distribution\n",
      "  Task: Operate office equipment, such as voice mail messaging systems, and use word processing, spreadsheet...\n",
      "\n",
      "[814] Librarians and Media Collections Specialists (93 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Compile lists of books, periodicals, articles, and audio-visual materials on particular subjects....\n",
      "\n",
      "[688] Computer Programmers (74 tasks)\n",
      "  Path: root/Technology & Engineering/Software Development/Software Maintenance and Enhancement\n",
      "  Task: Perform or direct revision, repair, or expansion of existing programs to increase operating efficien...\n",
      "\n",
      "[362] Business Intelligence Analysts (63 tasks)\n",
      "  Path: root/Records & Information Management/Document Services/Information Retrieval\n",
      "  Task: Collect business intelligence data from available industry reports, public information, field report...\n",
      "\n",
      "[439] Computer Programmers (58 tasks)\n",
      "  Path: root/Technology & Engineering/Software QA & Testing/Defect Remediation\n",
      "  Task: Correct errors by making appropriate changes and rechecking the program to ensure that the desired r...\n",
      "\n",
      "================================================================================\n",
      "METRIC 3: BENCHMARK COVERAGE (Greedy Set Cover)\n",
      "================================================================================\n",
      "\n",
      "Greedy Benchmark Selection (by marginal job coverage):\n",
      "\n",
      "Rank   Benchmark                 New Jobs     Total Jobs   Cumulative   Coverage %\n",
      "-------------------------------------------------------------------------------------\n",
      "1      gdpval                    255          255          255          37.9%\n",
      "2      ColBench                  52           100          307          45.6%\n",
      "3      flowbench                 37           53           344          51.1%\n",
      "4      toolbench                 26           90           370          55.0%\n",
      "5      TAC                       20           94           390          57.9%\n",
      "6      officebench               10           67           400          59.4%\n",
      "7      terminalbench             9            44           409          60.8%\n",
      "8      taskbench                 8            40           417          62.0%\n",
      "9      swebenchpro               8            16           425          63.2%\n",
      "10     galileo                   5            54           430          63.9%\n",
      "11     labbench                  3            8            433          64.3%\n",
      "12     crmarena                  2            14           435          64.6%\n",
      "13     osworld                   2            46           437          64.9%\n",
      "14     algotune                  2            10           439          65.2%\n",
      "15     webarena                  1            4            440          65.4%\n",
      "16     mind2web                  1            27           441          65.5%\n",
      "17     mldev                     1            4            442          65.7%\n",
      "18     bird                      1            13           443          65.8%\n",
      "19     mlebench                  1            10           444          66.0%\n",
      "20     swebenchm                 1            6            445          66.1%\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED BENCHMARK SELECTION\n",
      "================================================================================\n",
      "\n",
      "To cover 80% of jobs (538/673), use these benchmarks in order:\n",
      "1. gdpval (+255 new jobs)\n",
      "2. ColBench (+52 new jobs)\n",
      "3. flowbench (+37 new jobs)\n",
      "4. toolbench (+26 new jobs)\n",
      "5. TAC (+20 new jobs)\n",
      "6. officebench (+10 new jobs)\n",
      "7. terminalbench (+9 new jobs)\n",
      "8. taskbench (+8 new jobs)\n",
      "9. swebenchpro (+8 new jobs)\n",
      "10. galileo (+5 new jobs)\n",
      "11. labbench (+3 new jobs)\n",
      "12. crmarena (+2 new jobs)\n",
      "13. osworld (+2 new jobs)\n",
      "14. algotune (+2 new jobs)\n",
      "15. webarena (+1 new jobs)\n",
      "16. mind2web (+1 new jobs)\n",
      "17. mldev (+1 new jobs)\n",
      "18. bird (+1 new jobs)\n",
      "19. mlebench (+1 new jobs)\n",
      "20. swebenchm (+1 new jobs)\n",
      "\n",
      "Final coverage: 445/673 jobs (66.1%)\n",
      "\n",
      "================================================================================\n",
      "Results saved to: job_coverage_metrics.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate job coverage metrics from taxonomy with similarities.\n",
    "Metrics:\n",
    "1. Coverage: % of leaves with jobs that also have tasks\n",
    "2. Coverage Quality: For each job, count tasks with similarity >= threshold\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "TAXONOMY_FILE = \"taxonomy_with_similarity_updated.json\"\n",
    "OUTPUT_FILE = \"job_coverage_metrics.json\"\n",
    "SIMILARITY_THRESHOLD = 7  # Configurable: 5, 6, 7, etc.\n",
    "\n",
    "# -------------------------------\n",
    "# Helper functions\n",
    "# -------------------------------\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def find_all_leaves(node, path=[]):\n",
    "    \"\"\"Recursively find all leaf nodes.\"\"\"\n",
    "    leaves = []\n",
    "    if isinstance(node, dict):\n",
    "        has_tasks_or_jobs = \"tasks\" in node or \"jobs\" in node\n",
    "        is_leaf = has_tasks_or_jobs\n",
    "        \n",
    "        if is_leaf:\n",
    "            leaves.append({\n",
    "                \"path\": path,\n",
    "                \"tasks\": node.get(\"tasks\", []),\n",
    "                \"jobs\": node.get(\"jobs\", [])\n",
    "            })\n",
    "        \n",
    "        for k, v in node.items():\n",
    "            if k not in [\"tasks\", \"jobs\"]:\n",
    "                leaves.extend(find_all_leaves(v, path + [k]))\n",
    "    \n",
    "    return leaves\n",
    "\n",
    "# -------------------------------\n",
    "# Main analysis\n",
    "# -------------------------------\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"JOB COVERAGE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nSimilarity threshold: {SIMILARITY_THRESHOLD}\")\n",
    "    \n",
    "    # Load taxonomy\n",
    "    print(f\"Loading: {TAXONOMY_FILE}\")\n",
    "    taxonomy = load_json(TAXONOMY_FILE)\n",
    "    \n",
    "    # Find all leaves\n",
    "    all_leaves = find_all_leaves(taxonomy[\"root\"], [\"root\"])\n",
    "    print(f\"\\nTotal leaves: {len(all_leaves)}\")\n",
    "    \n",
    "    # Filter leaves with jobs\n",
    "    leaves_with_jobs = [\n",
    "        leaf for leaf in all_leaves \n",
    "        if len([j for j in leaf[\"jobs\"] if isinstance(j, dict)]) > 0\n",
    "    ]\n",
    "    print(f\"Leaves with jobs: {len(leaves_with_jobs)}\")\n",
    "    \n",
    "    # Calculate coverage: leaves with both jobs and tasks\n",
    "    leaves_with_both = [\n",
    "        leaf for leaf in leaves_with_jobs\n",
    "        if len([t for t in leaf[\"tasks\"] if isinstance(t, dict)]) > 0\n",
    "    ]\n",
    "    \n",
    "    coverage = len(leaves_with_both) / len(leaves_with_jobs) if leaves_with_jobs else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 1: COVERAGE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Leaves with jobs that also have tasks: {len(leaves_with_both)}/{len(leaves_with_jobs)}\")\n",
    "    print(f\"Coverage: {coverage:.2%}\")\n",
    "    \n",
    "    # Calculate coverage quality: job-level analysis\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 2: COVERAGE QUALITY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    job_coverage_details = []\n",
    "    total_jobs = 0\n",
    "    jobs_with_high_quality_tasks = 0\n",
    "    \n",
    "    for leaf in leaves_with_both:\n",
    "        path_str = \"/\".join(leaf[\"path\"])\n",
    "        \n",
    "        jobs = [j for j in leaf[\"jobs\"] if isinstance(j, dict)]\n",
    "        tasks = [t for t in leaf[\"tasks\"] if isinstance(t, dict)]\n",
    "        \n",
    "        for job in jobs:\n",
    "            total_jobs += 1\n",
    "            job_id = job[\"job_task_id\"]\n",
    "            \n",
    "            # Count tasks with high similarity to this job\n",
    "            high_sim_tasks = []\n",
    "            \n",
    "            for task in tasks:\n",
    "                similarities = task.get(\"job_similarities\", [])\n",
    "                for sim in similarities:\n",
    "                    if sim.get(\"job_task_id\") == job_id and sim.get(\"score\", 0) >= SIMILARITY_THRESHOLD:\n",
    "                        high_sim_tasks.append({\n",
    "                            \"task_id\": task[\"task_id\"],\n",
    "                            \"benchmark\": task[\"benchmark\"],\n",
    "                            \"score\": sim[\"score\"],\n",
    "                            \"reasoning\": sim.get(\"reasoning\", \"\")\n",
    "                        })\n",
    "            \n",
    "            if high_sim_tasks:\n",
    "                jobs_with_high_quality_tasks += 1\n",
    "            \n",
    "            job_coverage_details.append({\n",
    "                \"job_task_id\": job_id,\n",
    "                \"occupation\": job[\"occupation\"],\n",
    "                \"job_task\": job[\"task\"],\n",
    "                \"skill_path\": path_str,\n",
    "                \"num_high_quality_tasks\": len(high_sim_tasks),\n",
    "                \"high_quality_tasks\": high_sim_tasks\n",
    "            })\n",
    "    \n",
    "    quality_coverage = jobs_with_high_quality_tasks / total_jobs if total_jobs else 0\n",
    "    \n",
    "    print(f\"Total jobs analyzed: {total_jobs}\")\n",
    "    print(f\"Jobs with high-quality tasks (>={SIMILARITY_THRESHOLD}): {jobs_with_high_quality_tasks}\")\n",
    "    print(f\"Quality coverage: {quality_coverage:.2%}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    task_counts = [jc[\"num_high_quality_tasks\"] for jc in job_coverage_details]\n",
    "    task_count_dist = defaultdict(int)\n",
    "    for count in task_counts:\n",
    "        task_count_dist[count] += 1\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DISTRIBUTION: Tasks per Job\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for count in sorted(task_count_dist.keys()):\n",
    "        pct = task_count_dist[count] / total_jobs * 100 if total_jobs else 0\n",
    "        print(f\"{count} tasks: {task_count_dist[count]} jobs ({pct:.1f}%)\")\n",
    "    \n",
    "    # Jobs with no coverage\n",
    "    jobs_no_coverage = [jc for jc in job_coverage_details if jc[\"num_high_quality_tasks\"] == 0]\n",
    "    \n",
    "    if jobs_no_coverage:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"JOBS WITH NO HIGH-QUALITY TASKS: {len(jobs_no_coverage)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        for jc in jobs_no_coverage[:10]:  # Show first 10\n",
    "            print(f\"\\n[{jc['job_task_id']}] {jc['occupation']}\")\n",
    "            print(f\"  Path: {jc['skill_path']}\")\n",
    "            print(f\"  Task: {jc['job_task'][:100]}...\")\n",
    "        if len(jobs_no_coverage) > 10:\n",
    "            print(f\"\\n... and {len(jobs_no_coverage) - 10} more\")\n",
    "    \n",
    "    # Jobs with best coverage\n",
    "    jobs_best_coverage = sorted(job_coverage_details, key=lambda x: x[\"num_high_quality_tasks\"], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"JOBS WITH BEST COVERAGE (Top 10)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for jc in jobs_best_coverage:\n",
    "        print(f\"\\n[{jc['job_task_id']}] {jc['occupation']} ({jc['num_high_quality_tasks']} tasks)\")\n",
    "        print(f\"  Path: {jc['skill_path']}\")\n",
    "        print(f\"  Task: {jc['job_task'][:100]}...\")\n",
    "    \n",
    "    # Benchmark coverage analysis - GREEDY SET COVER\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"METRIC 3: BENCHMARK COVERAGE (Greedy Set Cover)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Build mapping: benchmark -> set of jobs it covers well\n",
    "    benchmark_to_jobs = defaultdict(set)\n",
    "    \n",
    "    for leaf in leaves_with_both:\n",
    "        jobs = [j for j in leaf[\"jobs\"] if isinstance(j, dict)]\n",
    "        tasks = [t for t in leaf[\"tasks\"] if isinstance(t, dict)]\n",
    "        \n",
    "        for task in tasks:\n",
    "            benchmark = task.get(\"benchmark\", \"unknown\")\n",
    "            similarities = task.get(\"job_similarities\", [])\n",
    "            \n",
    "            for sim in similarities:\n",
    "                if sim.get(\"score\", 0) >= SIMILARITY_THRESHOLD:\n",
    "                    job_id = sim.get(\"job_task_id\")\n",
    "                    benchmark_to_jobs[benchmark].add(job_id)\n",
    "    \n",
    "    # Greedy set cover: pick benchmark covering most uncovered jobs\n",
    "    covered_jobs = set()\n",
    "    benchmark_ranking = []\n",
    "    remaining_benchmarks = dict(benchmark_to_jobs)\n",
    "    \n",
    "    while remaining_benchmarks:\n",
    "        # Find benchmark that covers most NEW jobs\n",
    "        best_benchmark = None\n",
    "        best_new_coverage = set()\n",
    "        \n",
    "        for bench, jobs_covered in remaining_benchmarks.items():\n",
    "            new_jobs = jobs_covered - covered_jobs\n",
    "            if len(new_jobs) > len(best_new_coverage):\n",
    "                best_benchmark = bench\n",
    "                best_new_coverage = new_jobs\n",
    "        \n",
    "        if not best_benchmark or len(best_new_coverage) == 0:\n",
    "            break\n",
    "        \n",
    "        # Add to ranking\n",
    "        benchmark_ranking.append({\n",
    "            \"rank\": len(benchmark_ranking) + 1,\n",
    "            \"benchmark\": best_benchmark,\n",
    "            \"new_jobs_covered\": len(best_new_coverage),\n",
    "            \"total_jobs_covered\": len(remaining_benchmarks[best_benchmark]),\n",
    "            \"cumulative_coverage\": len(covered_jobs) + len(best_new_coverage)\n",
    "        })\n",
    "        \n",
    "        # Update covered jobs\n",
    "        covered_jobs.update(best_new_coverage)\n",
    "        del remaining_benchmarks[best_benchmark]\n",
    "    \n",
    "    print(f\"\\nGreedy Benchmark Selection (by marginal job coverage):\\n\")\n",
    "    print(f\"{'Rank':<6} {'Benchmark':<25} {'New Jobs':<12} {'Total Jobs':<12} {'Cumulative':<12} {'Coverage %'}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    for br in benchmark_ranking:\n",
    "        coverage_pct = br['cumulative_coverage'] / total_jobs * 100 if total_jobs else 0\n",
    "        print(f\"{br['rank']:<6} {br['benchmark']:<25} {br['new_jobs_covered']:<12} {br['total_jobs_covered']:<12} {br['cumulative_coverage']:<12} {coverage_pct:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RECOMMENDED BENCHMARK SELECTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find minimum set covering target percentage (e.g., 80%)\n",
    "    target_coverage = 0.80\n",
    "    target_jobs = int(target_coverage * total_jobs)\n",
    "    \n",
    "    recommended = []\n",
    "    cumulative = 0\n",
    "    for br in benchmark_ranking:\n",
    "        recommended.append(br['benchmark'])\n",
    "        cumulative = br['cumulative_coverage']\n",
    "        if cumulative >= target_jobs:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTo cover {target_coverage:.0%} of jobs ({target_jobs}/{total_jobs}), use these benchmarks in order:\")\n",
    "    for i, bench in enumerate(recommended, 1):\n",
    "        br = benchmark_ranking[i-1]\n",
    "        print(f\"{i}. {bench} (+{br['new_jobs_covered']} new jobs)\")\n",
    "    \n",
    "    final_coverage = cumulative / total_jobs if total_jobs else 0\n",
    "    print(f\"\\nFinal coverage: {cumulative}/{total_jobs} jobs ({final_coverage:.1%})\")\n",
    "    \n",
    "    # Save results\n",
    "    output = {\n",
    "        \"config\": {\n",
    "            \"similarity_threshold\": SIMILARITY_THRESHOLD\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"total_leaves\": len(all_leaves),\n",
    "            \"leaves_with_jobs\": len(leaves_with_jobs),\n",
    "            \"leaves_with_both\": len(leaves_with_both),\n",
    "            \"coverage\": coverage,\n",
    "            \"total_jobs\": total_jobs,\n",
    "            \"jobs_with_high_quality_tasks\": jobs_with_high_quality_tasks,\n",
    "            \"quality_coverage\": quality_coverage\n",
    "        },\n",
    "        \"distribution\": dict(task_count_dist),\n",
    "        \"benchmark_ranking\": benchmark_ranking,\n",
    "        \"recommended_benchmarks\": recommended,\n",
    "        \"job_details\": job_coverage_details\n",
    "    }\n",
    "    \n",
    "    save_json(output, OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Results saved to: {OUTPUT_FILE}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c762202c-cd55-40c3-ac89-bd9c575edf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANALYZING RESTRUCTURED TAXONOMY\n",
      "======================================================================\n",
      "Loading sentence embedding model (this may take a moment)...\n",
      "Generating embeddings for 844 tasks (leaf nodes only)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2912d12a2645518dce13008c476926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings ready for 844 tasks in leaf nodes\n",
      "  [Debug] Found 321 intermediate nodes\n",
      "  [Debug] Sample intermediate: ['Operations & Scheduling', 'Operations & Scheduling > Scheduling & Calendar Management', 'Operations & Scheduling > Scheduling & Calendar Management > Workforce Scheduling']\n",
      "  [Debug] Evaluated 61 intermediate nodes with descendants\n",
      "\n",
      "======================================================================\n",
      "TAXONOMY QUALITY ANALYSIS REPORT\n",
      "======================================================================\n",
      "\n",
      "📊 STRUCTURAL METRICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Depth Metrics:\n",
      "  Average Depth: 2.35\n",
      "  Max Depth: 5\n",
      "  Min Depth: 0\n",
      "  Std Depth: 0.81\n",
      "  Total Nodes: 643\n",
      "  Leaf Nodes: 322\n",
      "\n",
      "Branching Metrics:\n",
      "  Avg Branching Factor: 1.96\n",
      "  Max Branching Factor: 19.00\n",
      "  Min Branching Factor: 1.00\n",
      "  Std Branching Factor: 2.69\n",
      "\n",
      "Balance Coefficient (lower = better): 0.000\n",
      "\n",
      "Structural Complexity:\n",
      "  Total Nodes: 643\n",
      "  Leaf Nodes: 322\n",
      "  Intermediate Nodes: 321\n",
      "  Intermediate To Leaf Ratio: 1.00\n",
      "\n",
      "📋 TASK DISTRIBUTION\n",
      "----------------------------------------------------------------------\n",
      "  Total Tasks: 844\n",
      "  Categories With Tasks: 277\n",
      "  Total Leaf Nodes: 322\n",
      "  Coverage Rate: 0.860\n",
      "  Avg Tasks Per Category: 3.047\n",
      "  Max Tasks Per Category: 21\n",
      "  Min Tasks Per Category: 1\n",
      "  Std Tasks Per Category: 2.960\n",
      "  Gini Coefficient: 0.450\n",
      "\n",
      "🧠 SEMANTIC COHERENCE (Does the taxonomy make sense?)\n",
      "----------------------------------------------------------------------\n",
      "  Intra-Category Coherence: 0.494\n",
      "    → Higher = tasks within categories are more similar\n",
      "  Inter-Category Separation: 0.636\n",
      "    → Higher = categories are more distinct\n",
      "  Categories Analyzed: 163\n",
      "\n",
      "🎯 HIERARCHICAL CONSISTENCY\n",
      "----------------------------------------------------------------------\n",
      "  Avg Parent-Child Similarity: 0.510\n",
      "    → Higher = children are related to parents\n",
      "  Parent-Child Relationships Analyzed: 48\n",
      "\n",
      "👥 SIBLING COHERENCE (Are categories grouped well?)\n",
      "----------------------------------------------------------------------\n",
      "  Avg Sibling Similarity: 0.450\n",
      "    → Higher = siblings under same parent are more related\n",
      "  Sibling Pairs Analyzed: 1065\n",
      "  Parents with Multiple Children: 41\n",
      "\n",
      "🌲 INTERMEDIATE NODE QUALITY\n",
      "----------------------------------------------------------------------\n",
      "  Avg Intermediate Node Cohesion: 0.408\n",
      "    → Higher = better grouping at intermediate levels\n",
      "  Intermediate Nodes Evaluated: 61\n",
      "  Avg Leaf Descendants per Intermediate: 9.4\n",
      "  Avg Tasks under Intermediate Nodes: 28.4\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: ORIGINAL vs RESTRUCTURED\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ANALYZING ORIGINAL TAXONOMY\n",
      "======================================================================\n",
      "Loading sentence embedding model (this may take a moment)...\n",
      "⚠️  Warning: 51 tasks assigned to non-leaf nodes (will be excluded)\n",
      "    Non-leaf: Technical Support > Network Administration\n",
      "    Non-leaf: Financial Management > Accounts Payable\n",
      "    Non-leaf: Operations Management > HR Program Administration\n",
      "    Non-leaf: Operations Management > Production Administration\n",
      "    Non-leaf: Operations Management > Office Administration\n",
      "Generating embeddings for 793 tasks (leaf nodes only)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c0d04a9b4e47d09637a1329063e3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings ready for 793 tasks in leaf nodes\n",
      "  [Debug] Found 27 intermediate nodes\n",
      "  [Debug] Sample intermediate: ['Scheduling', 'Record Keeping', 'Record Keeping > Insurance Claims Documentation']\n",
      "  [Debug] Evaluated 19 intermediate nodes with descendants\n",
      "\n",
      "======================================================================\n",
      "ANALYZING RESTRUCTURED TAXONOMY\n",
      "======================================================================\n",
      "Loading sentence embedding model (this may take a moment)...\n",
      "Generating embeddings for 844 tasks (leaf nodes only)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547a3344de7f464d8f7a14a543be5867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Embeddings ready for 844 tasks in leaf nodes\n",
      "  [Debug] Found 321 intermediate nodes\n",
      "  [Debug] Sample intermediate: ['Operations & Scheduling', 'Operations & Scheduling > Scheduling & Calendar Management', 'Operations & Scheduling > Scheduling & Calendar Management > Workforce Scheduling']\n",
      "  [Debug] Evaluated 61 intermediate nodes with descendants\n",
      "\n",
      "======================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "✅ Average Depth: 1.12 → 2.35 (+1.23)\n",
      "✅ Intermediate Nodes: 27 → 321 (+294)\n",
      "✅ Total Nodes: 286 → 643 (+357)\n",
      "✅ Balance (lower=better): 0.000 → 0.000 (+0.000)\n",
      "✅ Intra-Category Coherence: 0.494 → 0.494 (-0.000)\n",
      "✅ Inter-Category Separation: 0.640 → 0.636 (-0.004)\n",
      "\n",
      "🔥 KEY IMPROVEMENT: Sibling Coherence\n",
      "✅ Avg Sibling Similarity: 0.386 → 0.450 (+0.064)\n",
      "   → This shows restructuring grouped related categories together!\n",
      "\n",
      "🔥 KEY IMPROVEMENT: Intermediate Node Quality\n",
      "✅ Intermediate Node Cohesion: 0.397 → 0.408 (+0.011)\n",
      "✅ Intermediate Nodes Evaluated: 19 → 61 (+42)\n",
      "   → More organizational structure with quality groupings!\n",
      "\n",
      "======================================================================\n",
      "📊 SUMMARY\n",
      "======================================================================\n",
      "✓ Added 294 intermediate nodes for better organization\n",
      "✓ Increased average depth by 1.23 levels (more specificity)\n",
      "✓ Improved sibling coherence by 0.064 (better groupings)\n",
      "✓ Overall taxonomy size increased by 124.8%\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "\n",
    "class TaxonomyAnalyzer:\n",
    "    \"\"\"Analyze taxonomy quality using embeddings and statistical metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, taxonomy_path: str, jobs_path: str = None):\n",
    "        \"\"\"Initialize analyzer with taxonomy and job data.\"\"\"\n",
    "        with open(taxonomy_path, 'r') as f:\n",
    "            self.taxonomy = json.load(f)\n",
    "        \n",
    "        self.jobs_df = None\n",
    "        self.embeddings = None\n",
    "        self.embedding_model = None\n",
    "        \n",
    "        if jobs_path:\n",
    "            self.jobs_df = pd.read_csv(jobs_path)\n",
    "            print(\"Loading sentence embedding model (this may take a moment)...\")\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self._prepare_embeddings()\n",
    "    \n",
    "    def _prepare_embeddings(self):\n",
    "        \"\"\"Create embeddings for all tasks.\"\"\"\n",
    "        if 'Task' not in self.jobs_df.columns or 'generic_skill' not in self.jobs_df.columns:\n",
    "            print(f\"Warning: Required columns not found. Available columns: {self.jobs_df.columns.tolist()}\")\n",
    "            return\n",
    "        \n",
    "        # Filter valid rows\n",
    "        valid_rows = self.jobs_df[\n",
    "            self.jobs_df['Task'].notna() & \n",
    "            self.jobs_df['generic_skill'].notna()\n",
    "        ].copy()\n",
    "        \n",
    "        if len(valid_rows) == 0:\n",
    "            print(\"Warning: No valid task-skill pairs found\")\n",
    "            return\n",
    "        \n",
    "        # Extract taxonomy paths\n",
    "        valid_rows['taxonomy_path'] = valid_rows['generic_skill'].apply(self._extract_taxonomy_path)\n",
    "        valid_rows = valid_rows[valid_rows['taxonomy_path'].notna()]\n",
    "        \n",
    "        if len(valid_rows) == 0:\n",
    "            print(\"Warning: No valid taxonomy paths extracted\")\n",
    "            return\n",
    "        \n",
    "        # Get all paths to determine leaf nodes\n",
    "        all_paths = self.get_all_paths()\n",
    "        \n",
    "        # Filter to only leaf nodes\n",
    "        valid_rows['is_leaf'] = valid_rows['taxonomy_path'].apply(\n",
    "            lambda p: self._is_leaf_from_set(p, all_paths)\n",
    "        )\n",
    "        \n",
    "        non_leaf_count = (~valid_rows['is_leaf']).sum()\n",
    "        if non_leaf_count > 0:\n",
    "            print(f\"⚠️  Warning: {non_leaf_count} tasks assigned to non-leaf nodes (will be excluded)\")\n",
    "            # Show examples\n",
    "            non_leaf_examples = valid_rows[~valid_rows['is_leaf']]['taxonomy_path'].unique()[:5]\n",
    "            for ex in non_leaf_examples:\n",
    "                print(f\"    Non-leaf: {ex}\")\n",
    "        \n",
    "        # Keep only leaf node assignments\n",
    "        valid_rows = valid_rows[valid_rows['is_leaf']].copy()\n",
    "        \n",
    "        if len(valid_rows) == 0:\n",
    "            print(\"Warning: No tasks assigned to leaf nodes\")\n",
    "            return\n",
    "        \n",
    "        # Generate embeddings\n",
    "        print(f\"Generating embeddings for {len(valid_rows)} tasks (leaf nodes only)...\")\n",
    "        tasks = valid_rows['Task'].tolist()\n",
    "        self.embeddings = self.embedding_model.encode(tasks, show_progress_bar=True)\n",
    "        \n",
    "        # Store with taxonomy mapping\n",
    "        valid_rows['embedding_idx'] = range(len(valid_rows))\n",
    "        self.jobs_df = valid_rows\n",
    "        \n",
    "        print(f\"✓ Embeddings ready for {len(valid_rows)} tasks in leaf nodes\")\n",
    "    \n",
    "    def _is_leaf_from_set(self, path: str, all_paths: List[str]) -> bool:\n",
    "        \"\"\"Check if a path is a leaf node given a set of all paths.\"\"\"\n",
    "        return not any(p.startswith(path + ' > ') for p in all_paths if p != path)\n",
    "    \n",
    "    def _extract_taxonomy_path(self, skill_text: str) -> str:\n",
    "        \"\"\"Extract taxonomy path from generic skill column.\"\"\"\n",
    "        if pd.isna(skill_text) or '(' not in skill_text or ')' not in skill_text:\n",
    "            return None\n",
    "        try:\n",
    "            path = skill_text.split('(')[1].split(')')[0].strip()\n",
    "            # Normalize path: remove leading/trailing spaces, normalize separators\n",
    "            path = ' > '.join([p.strip() for p in path.split('>')])\n",
    "            return path\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_all_paths(self, node: Dict = None, current_path: str = \"\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Recursively get all paths in taxonomy.\n",
    "        Returns paths WITHOUT 'root' prefix to match CSV format.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.taxonomy\n",
    "            # Start recursion - handle root node\n",
    "            if 'root' in node:\n",
    "                return self.get_all_paths(node['root'], \"\")\n",
    "            else:\n",
    "                return self.get_all_paths(node, \"\")\n",
    "        \n",
    "        paths = []\n",
    "        if current_path:  # Don't include empty string as a path\n",
    "            paths.append(current_path)\n",
    "        \n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key != \"root\":\n",
    "                    new_path = f\"{current_path} > {key}\" if current_path else key\n",
    "                    paths.extend(self.get_all_paths(value, new_path))\n",
    "        \n",
    "        return paths if paths else [\"\"]  # Return at least empty for root\n",
    "    \n",
    "    def _is_leaf(self, path: str) -> bool:\n",
    "        \"\"\"Check if a path is a leaf node.\"\"\"\n",
    "        all_paths = self.get_all_paths()\n",
    "        # A leaf has no children (no paths that start with \"path > \")\n",
    "        if not path:  # Empty path (root)\n",
    "            return False\n",
    "        return not any(p.startswith(path + ' > ') for p in all_paths if p != path)\n",
    "    \n",
    "    # ========== STRUCTURAL METRICS ==========\n",
    "    \n",
    "    def calculate_depth_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate depth-related metrics.\"\"\"\n",
    "        all_paths = self.get_all_paths()\n",
    "        depths = [path.count(' > ') for path in all_paths]\n",
    "        \n",
    "        return {\n",
    "            'average_depth': np.mean(depths),\n",
    "            'max_depth': max(depths),\n",
    "            'min_depth': min(depths),\n",
    "            'std_depth': np.std(depths),\n",
    "            'total_nodes': len(all_paths),\n",
    "            'leaf_nodes': sum(1 for p in all_paths if self._is_leaf(p))\n",
    "        }\n",
    "    \n",
    "    def calculate_branching_factor(self) -> Dict:\n",
    "        \"\"\"Calculate branching factor statistics.\"\"\"\n",
    "        def get_children_counts(node, path=\"root\"):\n",
    "            counts = []\n",
    "            if isinstance(node, dict):\n",
    "                for key, value in node.items():\n",
    "                    if key != \"root\":\n",
    "                        child_count = len(value) if isinstance(value, dict) else 0\n",
    "                        if child_count > 0:\n",
    "                            counts.append(child_count)\n",
    "                        counts.extend(get_children_counts(value, f\"{path}/{key}\"))\n",
    "                    else:\n",
    "                        counts.extend(get_children_counts(value, path))\n",
    "            return counts\n",
    "        \n",
    "        children_counts = get_children_counts(self.taxonomy)\n",
    "        if not children_counts:\n",
    "            return {'avg_branching_factor': 0, 'max_branching_factor': 0}\n",
    "        \n",
    "        return {\n",
    "            'avg_branching_factor': np.mean(children_counts),\n",
    "            'max_branching_factor': max(children_counts),\n",
    "            'min_branching_factor': min(children_counts),\n",
    "            'std_branching_factor': np.std(children_counts)\n",
    "        }\n",
    "    \n",
    "    def calculate_balance_metric(self) -> float:\n",
    "        \"\"\"Calculate tree balance (coefficient of variation: lower is more balanced).\"\"\"\n",
    "        all_paths = self.get_all_paths()\n",
    "        depths = [path.count('/') for path in all_paths]\n",
    "        return np.std(depths) / np.mean(depths) if np.mean(depths) > 0 else 0\n",
    "    \n",
    "    # ========== TASK DISTRIBUTION METRICS ==========\n",
    "    \n",
    "    def calculate_task_distribution_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate how tasks are distributed across taxonomy.\"\"\"\n",
    "        if self.jobs_df is None or len(self.jobs_df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        task_counts = self.jobs_df['taxonomy_path'].value_counts()\n",
    "        all_leaf_paths = [p for p in self.get_all_paths() if self._is_leaf(p)]\n",
    "        \n",
    "        # Coverage\n",
    "        coverage = len(task_counts) / len(all_leaf_paths) if len(all_leaf_paths) > 0 else 0\n",
    "        \n",
    "        # Distribution evenness (Gini coefficient)\n",
    "        counts = task_counts.values\n",
    "        gini = self._gini_coefficient(counts)\n",
    "        \n",
    "        return {\n",
    "            'total_tasks': len(self.jobs_df),\n",
    "            'categories_with_tasks': len(task_counts),\n",
    "            'total_leaf_nodes': len(all_leaf_paths),\n",
    "            'coverage_rate': coverage,\n",
    "            'avg_tasks_per_category': np.mean(counts),\n",
    "            'max_tasks_per_category': max(counts),\n",
    "            'min_tasks_per_category': min(counts),\n",
    "            'std_tasks_per_category': np.std(counts),\n",
    "            'gini_coefficient': gini\n",
    "        }\n",
    "    \n",
    "    def _gini_coefficient(self, values: np.ndarray) -> float:\n",
    "        \"\"\"Calculate Gini coefficient (0 = perfect equality, 1 = perfect inequality).\"\"\"\n",
    "        sorted_values = sorted(values)\n",
    "        n = len(sorted_values)\n",
    "        cumsum = np.cumsum(sorted_values)\n",
    "        return (2 * sum((i + 1) * val for i, val in enumerate(sorted_values))) / (n * sum(sorted_values)) - (n + 1) / n\n",
    "    \n",
    "    # ========== SEMANTIC COHERENCE METRICS ==========\n",
    "    \n",
    "    def calculate_semantic_coherence(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate semantic coherence using embeddings.\n",
    "        Measures if tasks in same category are similar, and different categories are distinct.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.jobs_df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Group embeddings by category\n",
    "        category_embeddings = defaultdict(list)\n",
    "        category_indices = defaultdict(list)\n",
    "        \n",
    "        for idx, row in self.jobs_df.iterrows():\n",
    "            path = row['taxonomy_path']\n",
    "            emb_idx = row['embedding_idx']\n",
    "            category_embeddings[path].append(self.embeddings[emb_idx])\n",
    "            category_indices[path].append(emb_idx)\n",
    "        \n",
    "        # Filter categories with at least 2 tasks\n",
    "        valid_categories = {k: v for k, v in category_embeddings.items() if len(v) >= 2}\n",
    "        \n",
    "        if len(valid_categories) < 2:\n",
    "            return {'error': 'Need at least 2 categories with 2+ tasks each'}\n",
    "        \n",
    "        # Calculate intra-category coherence (within-category similarity)\n",
    "        intra_coherence_scores = []\n",
    "        for category, embeddings in valid_categories.items():\n",
    "            embeddings_array = np.array(embeddings)\n",
    "            sim_matrix = cosine_similarity(embeddings_array)\n",
    "            # Average pairwise similarity (excluding diagonal)\n",
    "            n = len(embeddings)\n",
    "            intra_sim = (sim_matrix.sum() - n) / (n * (n - 1)) if n > 1 else 0\n",
    "            intra_coherence_scores.append(intra_sim)\n",
    "        \n",
    "        # Calculate inter-category separation (between-category dissimilarity)\n",
    "        category_centroids = {}\n",
    "        for category, embeddings in valid_categories.items():\n",
    "            category_centroids[category] = np.mean(embeddings, axis=0)\n",
    "        \n",
    "        inter_distances = []\n",
    "        categories = list(category_centroids.keys())\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i + 1, len(categories)):\n",
    "                sim = cosine_similarity([category_centroids[categories[i]]], \n",
    "                                       [category_centroids[categories[j]]])[0][0]\n",
    "                inter_distances.append(1 - sim)  # Convert to distance\n",
    "        \n",
    "        # Clustering quality metrics\n",
    "        all_embeddings = self.embeddings\n",
    "        labels = []\n",
    "        label_map = {cat: idx for idx, cat in enumerate(valid_categories.keys())}\n",
    "        \n",
    "        for _, row in self.jobs_df.iterrows():\n",
    "            path = row['taxonomy_path']\n",
    "            if path in label_map:\n",
    "                labels.append(label_map[path])\n",
    "        \n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        try:\n",
    "            silhouette = silhouette_score(all_embeddings, labels, metric='cosine')\n",
    "            davies_bouldin = davies_bouldin_score(all_embeddings, labels)\n",
    "            calinski = calinski_harabasz_score(all_embeddings, labels)\n",
    "        except:\n",
    "            silhouette = davies_bouldin = calinski = None\n",
    "        \n",
    "        return {\n",
    "            'intra_category_coherence': np.mean(intra_coherence_scores),\n",
    "            'inter_category_separation': np.mean(inter_distances),\n",
    "            'silhouette_score': silhouette,\n",
    "            'davies_bouldin_index': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski,\n",
    "            'num_categories': len(valid_categories)\n",
    "        }\n",
    "    \n",
    "    def calculate_hierarchical_consistency(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Check if parent-child relationships make semantic sense.\n",
    "        Child categories should be semantically related to parent categories.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.jobs_df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Group embeddings by category\n",
    "        category_embeddings = defaultdict(list)\n",
    "        for idx, row in self.jobs_df.iterrows():\n",
    "            path = row['taxonomy_path']\n",
    "            emb_idx = row['embedding_idx']\n",
    "            category_embeddings[path].append(self.embeddings[emb_idx])\n",
    "        \n",
    "        # Find parent-child pairs\n",
    "        parent_child_similarities = []\n",
    "        \n",
    "        for child_path in category_embeddings.keys():\n",
    "            parts = child_path.split(' > ')\n",
    "            if len(parts) > 1:\n",
    "                parent_path = ' > '.join(parts[:-1])\n",
    "                if parent_path in category_embeddings:\n",
    "                    # Calculate centroid similarity\n",
    "                    parent_centroid = np.mean(category_embeddings[parent_path], axis=0)\n",
    "                    child_centroid = np.mean(category_embeddings[child_path], axis=0)\n",
    "                    similarity = cosine_similarity([parent_centroid], [child_centroid])[0][0]\n",
    "                    parent_child_similarities.append(similarity)\n",
    "        \n",
    "        if not parent_child_similarities:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'avg_parent_child_similarity': np.mean(parent_child_similarities),\n",
    "            'std_parent_child_similarity': np.std(parent_child_similarities),\n",
    "            'num_relationships': len(parent_child_similarities)\n",
    "        }\n",
    "    \n",
    "    def calculate_sibling_coherence(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure if sibling categories (sharing same parent) are semantically similar.\n",
    "        Higher similarity = better grouping under common parent.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.jobs_df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Group embeddings by category\n",
    "        category_embeddings = defaultdict(list)\n",
    "        for idx, row in self.jobs_df.iterrows():\n",
    "            path = row['taxonomy_path']\n",
    "            emb_idx = row['embedding_idx']\n",
    "            category_embeddings[path].append(self.embeddings[emb_idx])\n",
    "        \n",
    "        # Group categories by parent\n",
    "        parent_to_children = defaultdict(list)\n",
    "        for path in category_embeddings.keys():\n",
    "            parts = path.split(' > ')\n",
    "            if len(parts) > 1:\n",
    "                parent_path = ' > '.join(parts[:-1])\n",
    "                parent_to_children[parent_path].append(path)\n",
    "        \n",
    "        # Calculate sibling similarity\n",
    "        sibling_similarities = []\n",
    "        parents_with_multiple_children = 0\n",
    "        \n",
    "        for parent, children in parent_to_children.items():\n",
    "            if len(children) >= 2:\n",
    "                parents_with_multiple_children += 1\n",
    "                # Calculate centroids for each child\n",
    "                child_centroids = []\n",
    "                for child in children:\n",
    "                    if len(category_embeddings[child]) > 0:\n",
    "                        centroid = np.mean(category_embeddings[child], axis=0)\n",
    "                        child_centroids.append(centroid)\n",
    "                \n",
    "                # Calculate pairwise similarity between siblings\n",
    "                if len(child_centroids) >= 2:\n",
    "                    for i in range(len(child_centroids)):\n",
    "                        for j in range(i + 1, len(child_centroids)):\n",
    "                            sim = cosine_similarity([child_centroids[i]], [child_centroids[j]])[0][0]\n",
    "                            sibling_similarities.append(sim)\n",
    "        \n",
    "        if not sibling_similarities:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'avg_sibling_similarity': np.mean(sibling_similarities),\n",
    "            'std_sibling_similarity': np.std(sibling_similarities),\n",
    "            'num_sibling_pairs': len(sibling_similarities),\n",
    "            'parents_with_multiple_children': parents_with_multiple_children\n",
    "        }\n",
    "    \n",
    "    def calculate_structural_complexity(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate structural complexity metrics.\n",
    "        Measures the organizational structure independent of task assignments.\n",
    "        \"\"\"\n",
    "        all_paths = self.get_all_paths()\n",
    "        \n",
    "        # Count leaf vs non-leaf nodes\n",
    "        leaf_nodes = [p for p in all_paths if self._is_leaf(p)]\n",
    "        non_leaf_nodes = [p for p in all_paths if not self._is_leaf(p) and p != 'root']\n",
    "        \n",
    "        # Depth distribution\n",
    "        depth_distribution = {}\n",
    "        for path in all_paths:\n",
    "            depth = path.count(' > ')\n",
    "            depth_distribution[depth] = depth_distribution.get(depth, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_nodes': len(all_paths),\n",
    "            'leaf_nodes': len(leaf_nodes),\n",
    "            'intermediate_nodes': len(non_leaf_nodes),\n",
    "            'intermediate_to_leaf_ratio': len(non_leaf_nodes) / len(leaf_nodes) if len(leaf_nodes) > 0 else 0,\n",
    "            'depth_distribution': depth_distribution\n",
    "        }\n",
    "    \n",
    "    def calculate_intermediate_node_quality(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Measure quality of intermediate (non-leaf) nodes.\n",
    "        Good intermediate nodes should group semantically similar children.\n",
    "        NOTE: This uses embeddings from leaf nodes to measure intermediate node quality.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None or len(self.jobs_df) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Group embeddings by LEAF category\n",
    "        leaf_category_embeddings = defaultdict(list)\n",
    "        for idx, row in self.jobs_df.iterrows():\n",
    "            path = row['taxonomy_path']\n",
    "            emb_idx = row['embedding_idx']\n",
    "            leaf_category_embeddings[path].append(self.embeddings[emb_idx])\n",
    "        \n",
    "        # Get all paths from taxonomy\n",
    "        all_paths = self.get_all_paths()\n",
    "        \n",
    "        # Find intermediate nodes (non-empty paths that are not leaves)\n",
    "        intermediate_nodes = []\n",
    "        for path in all_paths:\n",
    "            if path and not self._is_leaf(path):  # Non-empty and not a leaf\n",
    "                intermediate_nodes.append(path)\n",
    "        \n",
    "        if not intermediate_nodes:\n",
    "            return {'error': 'No intermediate nodes found'}\n",
    "        \n",
    "        print(f\"  [Debug] Found {len(intermediate_nodes)} intermediate nodes\")\n",
    "        print(f\"  [Debug] Sample intermediate: {intermediate_nodes[:3] if intermediate_nodes else 'none'}\")\n",
    "        \n",
    "        intermediate_quality_scores = []\n",
    "        \n",
    "        for intermediate_path in intermediate_nodes:\n",
    "            # Find all leaf descendants of this intermediate node\n",
    "            # Try both with and without \"root > \" prefix to handle both formats\n",
    "            \n",
    "            # Pattern 1: Direct match (for CSV without root prefix)\n",
    "            leaf_descendants = [\n",
    "                p for p in leaf_category_embeddings.keys() \n",
    "                if p.startswith(intermediate_path + ' > ') or p == intermediate_path\n",
    "            ]\n",
    "            \n",
    "            # Pattern 2: With root prefix (for CSV with root prefix)\n",
    "            if not leaf_descendants or len(leaf_descendants) < 2:\n",
    "                intermediate_with_root = f\"root > {intermediate_path}\"\n",
    "                leaf_descendants = [\n",
    "                    p for p in leaf_category_embeddings.keys() \n",
    "                    if p.startswith(intermediate_with_root + ' > ') or p == intermediate_with_root\n",
    "                ]\n",
    "            \n",
    "            if len(leaf_descendants) >= 2:\n",
    "                # Collect embeddings from all leaf descendants\n",
    "                all_descendant_embeddings = []\n",
    "                for leaf in leaf_descendants:\n",
    "                    all_descendant_embeddings.extend(leaf_category_embeddings[leaf])\n",
    "                \n",
    "                if len(all_descendant_embeddings) >= 2:\n",
    "                    # Calculate cohesion of all tasks under this intermediate node\n",
    "                    embeddings_array = np.array(all_descendant_embeddings)\n",
    "                    \n",
    "                    # Sample if too many (for efficiency)\n",
    "                    if len(embeddings_array) > 100:\n",
    "                        indices = np.random.choice(len(embeddings_array), 100, replace=False)\n",
    "                        embeddings_array = embeddings_array[indices]\n",
    "                    \n",
    "                    sim_matrix = cosine_similarity(embeddings_array)\n",
    "                    n = len(embeddings_array)\n",
    "                    cohesion = (sim_matrix.sum() - n) / (n * (n - 1)) if n > 1 else 0\n",
    "                    \n",
    "                    intermediate_quality_scores.append({\n",
    "                        'node': intermediate_path.split(' > ')[-1] if ' > ' in intermediate_path else intermediate_path,\n",
    "                        'full_path': intermediate_path,\n",
    "                        'depth': intermediate_path.count(' > '),\n",
    "                        'num_leaf_descendants': len(leaf_descendants),\n",
    "                        'num_tasks': len(all_descendant_embeddings),\n",
    "                        'cohesion': cohesion\n",
    "                    })\n",
    "        \n",
    "        if not intermediate_quality_scores:\n",
    "            print(f\"  [Debug] No intermediate nodes with 2+ leaf descendants found\")\n",
    "            print(f\"  [Debug] Leaf categories: {list(leaf_category_embeddings.keys())[:5]}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"  [Debug] Evaluated {len(intermediate_quality_scores)} intermediate nodes with descendants\")\n",
    "        \n",
    "        cohesion_scores = [s['cohesion'] for s in intermediate_quality_scores]\n",
    "        \n",
    "        return {\n",
    "            'avg_intermediate_node_cohesion': np.mean(cohesion_scores),\n",
    "            'std_intermediate_node_cohesion': np.std(cohesion_scores),\n",
    "            'num_intermediate_nodes_evaluated': len(intermediate_quality_scores),\n",
    "            'avg_leaf_descendants': np.mean([s['num_leaf_descendants'] for s in intermediate_quality_scores]),\n",
    "            'avg_tasks_under_intermediate': np.mean([s['num_tasks'] for s in intermediate_quality_scores]),\n",
    "            'max_cohesion': max(cohesion_scores),\n",
    "            'min_cohesion': min(cohesion_scores)\n",
    "        }\n",
    "    \n",
    "    # ========== REPORTING ==========\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate comprehensive quality metrics report.\"\"\"\n",
    "        report = {\n",
    "            'structural': {\n",
    "                'depth': self.calculate_depth_metrics(),\n",
    "                'branching': self.calculate_branching_factor(),\n",
    "                'balance': self.calculate_balance_metric(),\n",
    "                'complexity': self.calculate_structural_complexity()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if self.jobs_df is not None and len(self.jobs_df) > 0:\n",
    "            report['task_distribution'] = self.calculate_task_distribution_metrics()\n",
    "            report['semantic_coherence'] = self.calculate_semantic_coherence()\n",
    "            report['hierarchical_consistency'] = self.calculate_hierarchical_consistency()\n",
    "            report['sibling_coherence'] = self.calculate_sibling_coherence()\n",
    "            report['intermediate_node_quality'] = self.calculate_intermediate_node_quality()\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted analysis report.\"\"\"\n",
    "        report = self.generate_report()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TAXONOMY QUALITY ANALYSIS REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Structural metrics\n",
    "        print(\"\\n📊 STRUCTURAL METRICS\")\n",
    "        print(\"-\" * 70)\n",
    "        print(\"\\nDepth Metrics:\")\n",
    "        for key, value in report['structural']['depth'].items():\n",
    "            formatted_value = f\"{value:.2f}\" if isinstance(value, float) else str(value)\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {formatted_value}\")\n",
    "        \n",
    "        print(\"\\nBranching Metrics:\")\n",
    "        for key, value in report['structural']['branching'].items():\n",
    "            print(f\"  {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "        \n",
    "        print(f\"\\nBalance Coefficient (lower = better): {report['structural']['balance']:.3f}\")\n",
    "        \n",
    "        print(\"\\nStructural Complexity:\")\n",
    "        for key, value in report['structural']['complexity'].items():\n",
    "            if key != 'depth_distribution':\n",
    "                formatted_value = f\"{value:.2f}\" if isinstance(value, float) else str(value)\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {formatted_value}\")\n",
    "        \n",
    "        # Task distribution\n",
    "        if 'task_distribution' in report:\n",
    "            print(\"\\n📋 TASK DISTRIBUTION\")\n",
    "            print(\"-\" * 70)\n",
    "            for key, value in report['task_distribution'].items():\n",
    "                formatted_value = f\"{value:.3f}\" if isinstance(value, float) else str(value)\n",
    "                print(f\"  {key.replace('_', ' ').title()}: {formatted_value}\")\n",
    "        \n",
    "        # Semantic coherence\n",
    "        if 'semantic_coherence' in report and 'error' not in report['semantic_coherence']:\n",
    "            print(\"\\n🧠 SEMANTIC COHERENCE (Does the taxonomy make sense?)\")\n",
    "            print(\"-\" * 70)\n",
    "            sc = report['semantic_coherence']\n",
    "            \n",
    "            print(f\"  Intra-Category Coherence: {sc['intra_category_coherence']:.3f}\")\n",
    "            print(f\"    → Higher = tasks within categories are more similar\")\n",
    "            print(f\"  Inter-Category Separation: {sc['inter_category_separation']:.3f}\")\n",
    "            print(f\"    → Higher = categories are more distinct\")\n",
    "            \n",
    "            if sc['silhouette_score'] is not None:\n",
    "                print(f\"  Silhouette Score: {sc['silhouette_score']:.3f}\")\n",
    "                print(f\"    → Range [-1, 1], higher = better clustering\")\n",
    "            if sc['davies_bouldin_index'] is not None:\n",
    "                print(f\"  Davies-Bouldin Index: {sc['davies_bouldin_index']:.3f}\")\n",
    "                print(f\"    → Lower = better separated clusters\")\n",
    "            \n",
    "            print(f\"  Categories Analyzed: {sc['num_categories']}\")\n",
    "        \n",
    "        # Hierarchical consistency\n",
    "        if 'hierarchical_consistency' in report and report['hierarchical_consistency']:\n",
    "            print(\"\\n🎯 HIERARCHICAL CONSISTENCY\")\n",
    "            print(\"-\" * 70)\n",
    "            hc = report['hierarchical_consistency']\n",
    "            print(f\"  Avg Parent-Child Similarity: {hc['avg_parent_child_similarity']:.3f}\")\n",
    "            print(f\"    → Higher = children are related to parents\")\n",
    "            print(f\"  Parent-Child Relationships Analyzed: {hc['num_relationships']}\")\n",
    "        \n",
    "        # Sibling coherence\n",
    "        if 'sibling_coherence' in report and report['sibling_coherence']:\n",
    "            print(\"\\n👥 SIBLING COHERENCE (Are categories grouped well?)\")\n",
    "            print(\"-\" * 70)\n",
    "            sc = report['sibling_coherence']\n",
    "            print(f\"  Avg Sibling Similarity: {sc['avg_sibling_similarity']:.3f}\")\n",
    "            print(f\"    → Higher = siblings under same parent are more related\")\n",
    "            print(f\"  Sibling Pairs Analyzed: {sc['num_sibling_pairs']}\")\n",
    "            print(f\"  Parents with Multiple Children: {sc['parents_with_multiple_children']}\")\n",
    "        \n",
    "        # Intermediate node quality\n",
    "        if 'intermediate_node_quality' in report and report['intermediate_node_quality']:\n",
    "            print(\"\\n🌲 INTERMEDIATE NODE QUALITY\")\n",
    "            print(\"-\" * 70)\n",
    "            inq = report['intermediate_node_quality']\n",
    "            print(f\"  Avg Intermediate Node Cohesion: {inq['avg_intermediate_node_cohesion']:.3f}\")\n",
    "            print(f\"    → Higher = better grouping at intermediate levels\")\n",
    "            print(f\"  Intermediate Nodes Evaluated: {inq['num_intermediate_nodes_evaluated']}\")\n",
    "            print(f\"  Avg Leaf Descendants per Intermediate: {inq['avg_leaf_descendants']:.1f}\")\n",
    "            print(f\"  Avg Tasks under Intermediate Nodes: {inq['avg_tasks_under_intermediate']:.1f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "def compare_taxonomies(original_taxonomy: str, restructured_taxonomy: str, \n",
    "                       original_jobs: str, restructured_jobs: str):\n",
    "    \"\"\"Compare two taxonomies with their respective job mappings.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYZING ORIGINAL TAXONOMY\")\n",
    "    print(\"=\" * 70)\n",
    "    original = TaxonomyAnalyzer(original_taxonomy, original_jobs)\n",
    "    original_report = original.generate_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ANALYZING RESTRUCTURED TAXONOMY\")\n",
    "    print(\"=\" * 70)\n",
    "    restructured = TaxonomyAnalyzer(restructured_taxonomy, restructured_jobs)\n",
    "    restructured_report = restructured.generate_report()\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Depth improvement\n",
    "    orig_depth = original_report['structural']['depth']['average_depth']\n",
    "    new_depth = restructured_report['structural']['depth']['average_depth']\n",
    "    print(f\"\\n✅ Average Depth: {orig_depth:.2f} → {new_depth:.2f} ({new_depth - orig_depth:+.2f})\")\n",
    "    \n",
    "    # Structural complexity improvements\n",
    "    orig_intermediate = original_report['structural']['complexity']['intermediate_nodes']\n",
    "    new_intermediate = restructured_report['structural']['complexity']['intermediate_nodes']\n",
    "    print(f\"✅ Intermediate Nodes: {orig_intermediate} → {new_intermediate} ({new_intermediate - orig_intermediate:+d})\")\n",
    "    \n",
    "    orig_total = original_report['structural']['complexity']['total_nodes']\n",
    "    new_total = restructured_report['structural']['complexity']['total_nodes']\n",
    "    print(f\"✅ Total Nodes: {orig_total} → {new_total} ({new_total - orig_total:+d})\")\n",
    "    \n",
    "    # Balance improvement\n",
    "    orig_balance = original_report['structural']['balance']\n",
    "    new_balance = restructured_report['structural']['balance']\n",
    "    print(f\"✅ Balance (lower=better): {orig_balance:.3f} → {new_balance:.3f} ({orig_balance - new_balance:+.3f})\")\n",
    "    \n",
    "    # Semantic improvements\n",
    "    if ('semantic_coherence' in original_report and 'semantic_coherence' in restructured_report and\n",
    "        'error' not in original_report['semantic_coherence'] and \n",
    "        'error' not in restructured_report['semantic_coherence']):\n",
    "        \n",
    "        orig_coherence = original_report['semantic_coherence']['intra_category_coherence']\n",
    "        new_coherence = restructured_report['semantic_coherence']['intra_category_coherence']\n",
    "        print(f\"✅ Intra-Category Coherence: {orig_coherence:.3f} → {new_coherence:.3f} ({new_coherence - orig_coherence:+.3f})\")\n",
    "        \n",
    "        orig_sep = original_report['semantic_coherence']['inter_category_separation']\n",
    "        new_sep = restructured_report['semantic_coherence']['inter_category_separation']\n",
    "        print(f\"✅ Inter-Category Separation: {orig_sep:.3f} → {new_sep:.3f} ({new_sep - orig_sep:+.3f})\")\n",
    "        \n",
    "        if (original_report['semantic_coherence']['silhouette_score'] is not None and\n",
    "            restructured_report['semantic_coherence']['silhouette_score'] is not None):\n",
    "            orig_sil = original_report['semantic_coherence']['silhouette_score']\n",
    "            new_sil = restructured_report['semantic_coherence']['silhouette_score']\n",
    "            print(f\"✅ Silhouette Score: {orig_sil:.3f} → {new_sil:.3f} ({new_sil - orig_sil:+.3f})\")\n",
    "    \n",
    "    # Sibling coherence improvements\n",
    "    if ('sibling_coherence' in original_report and 'sibling_coherence' in restructured_report):\n",
    "        print(\"\\n🔥 KEY IMPROVEMENT: Sibling Coherence\")\n",
    "        orig_sib = original_report['sibling_coherence'].get('avg_sibling_similarity', 0)\n",
    "        new_sib = restructured_report['sibling_coherence'].get('avg_sibling_similarity', 0)\n",
    "        print(f\"✅ Avg Sibling Similarity: {orig_sib:.3f} → {new_sib:.3f} ({new_sib - orig_sib:+.3f})\")\n",
    "        print(f\"   → This shows restructuring grouped related categories together!\")\n",
    "    \n",
    "    # Intermediate node quality\n",
    "    if ('intermediate_node_quality' in original_report and 'intermediate_node_quality' in restructured_report):\n",
    "        print(\"\\n🔥 KEY IMPROVEMENT: Intermediate Node Quality\")\n",
    "        orig_inq = original_report['intermediate_node_quality'].get('avg_intermediate_node_cohesion', 0)\n",
    "        new_inq = restructured_report['intermediate_node_quality'].get('avg_intermediate_node_cohesion', 0)\n",
    "        print(f\"✅ Intermediate Node Cohesion: {orig_inq:.3f} → {new_inq:.3f} ({new_inq - orig_inq:+.3f})\")\n",
    "        \n",
    "        orig_num_with_tasks = original_report['intermediate_node_quality'].get('num_intermediate_nodes_evaluated', 0)\n",
    "        new_num_with_tasks = restructured_report['intermediate_node_quality'].get('num_intermediate_nodes_evaluated', 0)\n",
    "        print(f\"✅ Intermediate Nodes Evaluated: {orig_num_with_tasks} → {new_num_with_tasks} ({new_num_with_tasks - orig_num_with_tasks:+d})\")\n",
    "        print(f\"   → More organizational structure with quality groupings!\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📊 SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✓ Added {new_intermediate - orig_intermediate} intermediate nodes for better organization\")\n",
    "    print(f\"✓ Increased average depth by {new_depth - orig_depth:.2f} levels (more specificity)\")\n",
    "    \n",
    "    if 'sibling_coherence' in original_report and 'sibling_coherence' in restructured_report:\n",
    "        orig_sib = original_report['sibling_coherence'].get('avg_sibling_similarity', 0)\n",
    "        new_sib = restructured_report['sibling_coherence'].get('avg_sibling_similarity', 0)\n",
    "        if new_sib > orig_sib:\n",
    "            print(f\"✓ Improved sibling coherence by {new_sib - orig_sib:.3f} (better groupings)\")\n",
    "    \n",
    "    print(f\"✓ Overall taxonomy size increased by {((new_total - orig_total) / orig_total * 100):.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Analyze single taxonomy (restructured)\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANALYZING RESTRUCTURED TAXONOMY\")\n",
    "    print(\"=\" * 70)\n",
    "    analyzer = TaxonomyAnalyzer('../taxonomy_restructured.json', '../job_tasks_with_skills_restructured.csv')\n",
    "    analyzer.print_report()\n",
    "    \n",
    "    # Compare taxonomies\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPARISON: ORIGINAL vs RESTRUCTURED\")\n",
    "    print(\"=\" * 70)\n",
    "    compare_taxonomies(\n",
    "        original_taxonomy='../taxonomy.json',\n",
    "        restructured_taxonomy='../taxonomy_restructured.json',\n",
    "        original_jobs='../job_tasks_with_skills.csv',\n",
    "        restructured_jobs='../job_tasks_with_skills_restructured.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5bf35-5cd3-4e4e-986e-db5f2a32a98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
